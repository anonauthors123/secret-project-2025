{
  "best_metric": 0.026615800336003304,
  "best_model_checkpoint": "saves/source_code_mix_pseudo_code/qwen-7b/lora_sft/checkpoint-2221",
  "epoch": 2.9986492570914,
  "eval_steps": 500,
  "global_step": 3330,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009004952723998198,
      "grad_norm": 0.9248061776161194,
      "learning_rate": 3.003003003003003e-06,
      "loss": 1.9533,
      "step": 10
    },
    {
      "epoch": 0.018009905447996397,
      "grad_norm": 0.8116487264633179,
      "learning_rate": 6.006006006006006e-06,
      "loss": 1.7026,
      "step": 20
    },
    {
      "epoch": 0.0270148581719946,
      "grad_norm": 2.263157367706299,
      "learning_rate": 9.00900900900901e-06,
      "loss": 1.7263,
      "step": 30
    },
    {
      "epoch": 0.03601981089599279,
      "grad_norm": 1.1570444107055664,
      "learning_rate": 1.2012012012012012e-05,
      "loss": 1.833,
      "step": 40
    },
    {
      "epoch": 0.045024763619991,
      "grad_norm": 1.834645390510559,
      "learning_rate": 1.5015015015015016e-05,
      "loss": 1.8889,
      "step": 50
    },
    {
      "epoch": 0.0540297163439892,
      "grad_norm": 1.2088485956192017,
      "learning_rate": 1.801801801801802e-05,
      "loss": 1.426,
      "step": 60
    },
    {
      "epoch": 0.0630346690679874,
      "grad_norm": 0.8823980093002319,
      "learning_rate": 2.102102102102102e-05,
      "loss": 1.0921,
      "step": 70
    },
    {
      "epoch": 0.07203962179198559,
      "grad_norm": 0.906247615814209,
      "learning_rate": 2.4024024024024024e-05,
      "loss": 1.0025,
      "step": 80
    },
    {
      "epoch": 0.08104457451598379,
      "grad_norm": 1.2586764097213745,
      "learning_rate": 2.702702702702703e-05,
      "loss": 0.8037,
      "step": 90
    },
    {
      "epoch": 0.090049527239982,
      "grad_norm": 1.32680344581604,
      "learning_rate": 3.0030030030030033e-05,
      "loss": 0.6463,
      "step": 100
    },
    {
      "epoch": 0.09905447996398019,
      "grad_norm": 1.7190101146697998,
      "learning_rate": 3.3033033033033035e-05,
      "loss": 0.3782,
      "step": 110
    },
    {
      "epoch": 0.1080594326879784,
      "grad_norm": 5.4386138916015625,
      "learning_rate": 3.603603603603604e-05,
      "loss": 0.2625,
      "step": 120
    },
    {
      "epoch": 0.11706438541197658,
      "grad_norm": 0.4753958284854889,
      "learning_rate": 3.903903903903904e-05,
      "loss": 0.2334,
      "step": 130
    },
    {
      "epoch": 0.1260693381359748,
      "grad_norm": 0.5036231875419617,
      "learning_rate": 4.204204204204204e-05,
      "loss": 0.2197,
      "step": 140
    },
    {
      "epoch": 0.135074290859973,
      "grad_norm": 0.5941360592842102,
      "learning_rate": 4.5045045045045046e-05,
      "loss": 0.2188,
      "step": 150
    },
    {
      "epoch": 0.14407924358397117,
      "grad_norm": 0.9245783090591431,
      "learning_rate": 4.804804804804805e-05,
      "loss": 0.206,
      "step": 160
    },
    {
      "epoch": 0.15308419630796938,
      "grad_norm": 0.7386232614517212,
      "learning_rate": 5.105105105105106e-05,
      "loss": 0.1994,
      "step": 170
    },
    {
      "epoch": 0.16208914903196758,
      "grad_norm": 0.5720211267471313,
      "learning_rate": 5.405405405405406e-05,
      "loss": 0.2045,
      "step": 180
    },
    {
      "epoch": 0.1710941017559658,
      "grad_norm": 0.8049440979957581,
      "learning_rate": 5.705705705705706e-05,
      "loss": 0.1788,
      "step": 190
    },
    {
      "epoch": 0.180099054479964,
      "grad_norm": 3.2221086025238037,
      "learning_rate": 6.0060060060060066e-05,
      "loss": 0.2161,
      "step": 200
    },
    {
      "epoch": 0.18910400720396217,
      "grad_norm": 0.8387917280197144,
      "learning_rate": 6.306306306306306e-05,
      "loss": 0.1927,
      "step": 210
    },
    {
      "epoch": 0.19810895992796038,
      "grad_norm": 1.8841722011566162,
      "learning_rate": 6.606606606606607e-05,
      "loss": 0.1733,
      "step": 220
    },
    {
      "epoch": 0.20711391265195858,
      "grad_norm": 0.6124489307403564,
      "learning_rate": 6.906906906906907e-05,
      "loss": 0.1752,
      "step": 230
    },
    {
      "epoch": 0.2161188653759568,
      "grad_norm": 3.7114217281341553,
      "learning_rate": 7.207207207207208e-05,
      "loss": 0.1844,
      "step": 240
    },
    {
      "epoch": 0.22512381809995496,
      "grad_norm": 0.5723757147789001,
      "learning_rate": 7.507507507507507e-05,
      "loss": 0.1883,
      "step": 250
    },
    {
      "epoch": 0.23412877082395317,
      "grad_norm": 1.642683506011963,
      "learning_rate": 7.807807807807808e-05,
      "loss": 0.1506,
      "step": 260
    },
    {
      "epoch": 0.24313372354795137,
      "grad_norm": 1.0651353597640991,
      "learning_rate": 8.108108108108109e-05,
      "loss": 0.1772,
      "step": 270
    },
    {
      "epoch": 0.2521386762719496,
      "grad_norm": 1.0912706851959229,
      "learning_rate": 8.408408408408409e-05,
      "loss": 0.1828,
      "step": 280
    },
    {
      "epoch": 0.2611436289959478,
      "grad_norm": 1.12925124168396,
      "learning_rate": 8.70870870870871e-05,
      "loss": 0.1572,
      "step": 290
    },
    {
      "epoch": 0.270148581719946,
      "grad_norm": 0.5970382690429688,
      "learning_rate": 9.009009009009009e-05,
      "loss": 0.146,
      "step": 300
    },
    {
      "epoch": 0.2791535344439442,
      "grad_norm": 2.699188232421875,
      "learning_rate": 9.30930930930931e-05,
      "loss": 0.1426,
      "step": 310
    },
    {
      "epoch": 0.28815848716794235,
      "grad_norm": 0.6729522347450256,
      "learning_rate": 9.60960960960961e-05,
      "loss": 0.1608,
      "step": 320
    },
    {
      "epoch": 0.29716343989194055,
      "grad_norm": 1.1612939834594727,
      "learning_rate": 9.90990990990991e-05,
      "loss": 0.1508,
      "step": 330
    },
    {
      "epoch": 0.30616839261593876,
      "grad_norm": 1.6439534425735474,
      "learning_rate": 9.999865395245715e-05,
      "loss": 0.1498,
      "step": 340
    },
    {
      "epoch": 0.31517334533993696,
      "grad_norm": 0.6765783429145813,
      "learning_rate": 9.999206124100069e-05,
      "loss": 0.1669,
      "step": 350
    },
    {
      "epoch": 0.32417829806393517,
      "grad_norm": 0.6814252138137817,
      "learning_rate": 9.99799753559161e-05,
      "loss": 0.1377,
      "step": 360
    },
    {
      "epoch": 0.33318325078793337,
      "grad_norm": 1.3873865604400635,
      "learning_rate": 9.996239762521151e-05,
      "loss": 0.1316,
      "step": 370
    },
    {
      "epoch": 0.3421882035119316,
      "grad_norm": 0.5315461754798889,
      "learning_rate": 9.993932998034416e-05,
      "loss": 0.1164,
      "step": 380
    },
    {
      "epoch": 0.3511931562359298,
      "grad_norm": 0.5723503232002258,
      "learning_rate": 9.991077495600804e-05,
      "loss": 0.1458,
      "step": 390
    },
    {
      "epoch": 0.360198108959928,
      "grad_norm": 0.7198649048805237,
      "learning_rate": 9.987673568985549e-05,
      "loss": 0.1441,
      "step": 400
    },
    {
      "epoch": 0.36920306168392614,
      "grad_norm": 0.6637531518936157,
      "learning_rate": 9.983721592215234e-05,
      "loss": 0.1205,
      "step": 410
    },
    {
      "epoch": 0.37820801440792434,
      "grad_norm": 0.6721411943435669,
      "learning_rate": 9.979221999536699e-05,
      "loss": 0.1197,
      "step": 420
    },
    {
      "epoch": 0.38721296713192255,
      "grad_norm": 0.6854309439659119,
      "learning_rate": 9.974175285369321e-05,
      "loss": 0.0983,
      "step": 430
    },
    {
      "epoch": 0.39621791985592075,
      "grad_norm": 0.9662805795669556,
      "learning_rate": 9.96858200425069e-05,
      "loss": 0.1184,
      "step": 440
    },
    {
      "epoch": 0.40522287257991896,
      "grad_norm": 0.5039095282554626,
      "learning_rate": 9.962442770775675e-05,
      "loss": 0.1417,
      "step": 450
    },
    {
      "epoch": 0.41422782530391716,
      "grad_norm": 0.6057625412940979,
      "learning_rate": 9.955758259528894e-05,
      "loss": 0.1202,
      "step": 460
    },
    {
      "epoch": 0.42323277802791537,
      "grad_norm": 0.9465323686599731,
      "learning_rate": 9.948529205010583e-05,
      "loss": 0.1199,
      "step": 470
    },
    {
      "epoch": 0.4322377307519136,
      "grad_norm": 0.5544314384460449,
      "learning_rate": 9.940756401555898e-05,
      "loss": 0.1337,
      "step": 480
    },
    {
      "epoch": 0.4412426834759118,
      "grad_norm": 0.6239093542098999,
      "learning_rate": 9.932440703247623e-05,
      "loss": 0.1093,
      "step": 490
    },
    {
      "epoch": 0.45024763619990993,
      "grad_norm": 0.4434044361114502,
      "learning_rate": 9.92358302382233e-05,
      "loss": 0.1076,
      "step": 500
    },
    {
      "epoch": 0.45925258892390813,
      "grad_norm": 0.5450742244720459,
      "learning_rate": 9.914184336569974e-05,
      "loss": 0.1103,
      "step": 510
    },
    {
      "epoch": 0.46825754164790634,
      "grad_norm": 0.6397514939308167,
      "learning_rate": 9.904245674226947e-05,
      "loss": 0.1159,
      "step": 520
    },
    {
      "epoch": 0.47726249437190454,
      "grad_norm": 0.9488863348960876,
      "learning_rate": 9.893768128862601e-05,
      "loss": 0.1146,
      "step": 530
    },
    {
      "epoch": 0.48626744709590275,
      "grad_norm": 0.3685306906700134,
      "learning_rate": 9.882752851759248e-05,
      "loss": 0.0993,
      "step": 540
    },
    {
      "epoch": 0.49527239981990095,
      "grad_norm": 0.6161948442459106,
      "learning_rate": 9.871201053285657e-05,
      "loss": 0.1141,
      "step": 550
    },
    {
      "epoch": 0.5042773525438992,
      "grad_norm": 1.9086003303527832,
      "learning_rate": 9.859114002764061e-05,
      "loss": 0.1158,
      "step": 560
    },
    {
      "epoch": 0.5132823052678973,
      "grad_norm": 1.679491400718689,
      "learning_rate": 9.846493028330678e-05,
      "loss": 0.119,
      "step": 570
    },
    {
      "epoch": 0.5222872579918956,
      "grad_norm": 1.5775527954101562,
      "learning_rate": 9.833339516789776e-05,
      "loss": 0.1036,
      "step": 580
    },
    {
      "epoch": 0.5312922107158937,
      "grad_norm": 0.5030438899993896,
      "learning_rate": 9.819654913461292e-05,
      "loss": 0.087,
      "step": 590
    },
    {
      "epoch": 0.540297163439892,
      "grad_norm": 0.7119857668876648,
      "learning_rate": 9.805440722022014e-05,
      "loss": 0.0867,
      "step": 600
    },
    {
      "epoch": 0.5493021161638901,
      "grad_norm": 1.8162052631378174,
      "learning_rate": 9.790698504340358e-05,
      "loss": 0.1159,
      "step": 610
    },
    {
      "epoch": 0.5583070688878884,
      "grad_norm": 1.1955024003982544,
      "learning_rate": 9.775429880304752e-05,
      "loss": 0.1029,
      "step": 620
    },
    {
      "epoch": 0.5673120216118865,
      "grad_norm": 0.8961042761802673,
      "learning_rate": 9.759636527645633e-05,
      "loss": 0.0983,
      "step": 630
    },
    {
      "epoch": 0.5763169743358847,
      "grad_norm": 0.45013228058815,
      "learning_rate": 9.743320181751105e-05,
      "loss": 0.0916,
      "step": 640
    },
    {
      "epoch": 0.585321927059883,
      "grad_norm": 0.8477461338043213,
      "learning_rate": 9.72648263547625e-05,
      "loss": 0.0884,
      "step": 650
    },
    {
      "epoch": 0.5943268797838811,
      "grad_norm": 1.3654476404190063,
      "learning_rate": 9.709125738946125e-05,
      "loss": 0.1121,
      "step": 660
    },
    {
      "epoch": 0.6033318325078794,
      "grad_norm": 0.842921257019043,
      "learning_rate": 9.691251399352467e-05,
      "loss": 0.0878,
      "step": 670
    },
    {
      "epoch": 0.6123367852318775,
      "grad_norm": 0.9007009267807007,
      "learning_rate": 9.672861580744142e-05,
      "loss": 0.0963,
      "step": 680
    },
    {
      "epoch": 0.6213417379558758,
      "grad_norm": 0.4272764325141907,
      "learning_rate": 9.65395830381131e-05,
      "loss": 0.0996,
      "step": 690
    },
    {
      "epoch": 0.6303466906798739,
      "grad_norm": 0.9189590811729431,
      "learning_rate": 9.634543645663417e-05,
      "loss": 0.0947,
      "step": 700
    },
    {
      "epoch": 0.6393516434038722,
      "grad_norm": 0.5967751741409302,
      "learning_rate": 9.614619739600938e-05,
      "loss": 0.0926,
      "step": 710
    },
    {
      "epoch": 0.6483565961278703,
      "grad_norm": 0.4169386029243469,
      "learning_rate": 9.594188774880982e-05,
      "loss": 0.0866,
      "step": 720
    },
    {
      "epoch": 0.6573615488518685,
      "grad_norm": 0.5619782209396362,
      "learning_rate": 9.573252996476723e-05,
      "loss": 0.0852,
      "step": 730
    },
    {
      "epoch": 0.6663665015758667,
      "grad_norm": 0.9724687933921814,
      "learning_rate": 9.551814704830734e-05,
      "loss": 0.0932,
      "step": 740
    },
    {
      "epoch": 0.6753714542998649,
      "grad_norm": 0.7050515413284302,
      "learning_rate": 9.5298762556022e-05,
      "loss": 0.0856,
      "step": 750
    },
    {
      "epoch": 0.6843764070238632,
      "grad_norm": 0.9795717000961304,
      "learning_rate": 9.507440059408082e-05,
      "loss": 0.0722,
      "step": 760
    },
    {
      "epoch": 0.6933813597478613,
      "grad_norm": 0.8474711179733276,
      "learning_rate": 9.484508581558236e-05,
      "loss": 0.1133,
      "step": 770
    },
    {
      "epoch": 0.7023863124718596,
      "grad_norm": 0.865481436252594,
      "learning_rate": 9.46108434178452e-05,
      "loss": 0.0949,
      "step": 780
    },
    {
      "epoch": 0.7113912651958577,
      "grad_norm": 0.7648237943649292,
      "learning_rate": 9.437169913963923e-05,
      "loss": 0.0782,
      "step": 790
    },
    {
      "epoch": 0.720396217919856,
      "grad_norm": 0.8325797319412231,
      "learning_rate": 9.412767925835754e-05,
      "loss": 0.0901,
      "step": 800
    },
    {
      "epoch": 0.7294011706438541,
      "grad_norm": 0.5453308820724487,
      "learning_rate": 9.387881058712888e-05,
      "loss": 0.0752,
      "step": 810
    },
    {
      "epoch": 0.7384061233678523,
      "grad_norm": 2.7936956882476807,
      "learning_rate": 9.362512047187157e-05,
      "loss": 0.0765,
      "step": 820
    },
    {
      "epoch": 0.7474110760918505,
      "grad_norm": 0.6128243207931519,
      "learning_rate": 9.336663678828858e-05,
      "loss": 0.0906,
      "step": 830
    },
    {
      "epoch": 0.7564160288158487,
      "grad_norm": 0.801774799823761,
      "learning_rate": 9.310338793880458e-05,
      "loss": 0.0941,
      "step": 840
    },
    {
      "epoch": 0.765420981539847,
      "grad_norm": 0.5539017915725708,
      "learning_rate": 9.283540284944507e-05,
      "loss": 0.0751,
      "step": 850
    },
    {
      "epoch": 0.7744259342638451,
      "grad_norm": 0.3352106809616089,
      "learning_rate": 9.256271096665791e-05,
      "loss": 0.0685,
      "step": 860
    },
    {
      "epoch": 0.7834308869878434,
      "grad_norm": 1.0687133073806763,
      "learning_rate": 9.228534225407781e-05,
      "loss": 0.0922,
      "step": 870
    },
    {
      "epoch": 0.7924358397118415,
      "grad_norm": 0.7888761758804321,
      "learning_rate": 9.200332718923375e-05,
      "loss": 0.078,
      "step": 880
    },
    {
      "epoch": 0.8014407924358398,
      "grad_norm": 0.647070050239563,
      "learning_rate": 9.171669676020021e-05,
      "loss": 0.0765,
      "step": 890
    },
    {
      "epoch": 0.8104457451598379,
      "grad_norm": 0.5478561520576477,
      "learning_rate": 9.142548246219212e-05,
      "loss": 0.0799,
      "step": 900
    },
    {
      "epoch": 0.8194506978838361,
      "grad_norm": 0.3115473687648773,
      "learning_rate": 9.112971629410416e-05,
      "loss": 0.053,
      "step": 910
    },
    {
      "epoch": 0.8284556506078343,
      "grad_norm": 0.9373671412467957,
      "learning_rate": 9.082943075499466e-05,
      "loss": 0.0646,
      "step": 920
    },
    {
      "epoch": 0.8374606033318325,
      "grad_norm": 0.7379541397094727,
      "learning_rate": 9.05246588405146e-05,
      "loss": 0.0864,
      "step": 930
    },
    {
      "epoch": 0.8464655560558307,
      "grad_norm": 1.1135339736938477,
      "learning_rate": 9.021543403928202e-05,
      "loss": 0.0722,
      "step": 940
    },
    {
      "epoch": 0.8554705087798289,
      "grad_norm": 0.4138835668563843,
      "learning_rate": 8.990179032920222e-05,
      "loss": 0.0783,
      "step": 950
    },
    {
      "epoch": 0.8644754615038271,
      "grad_norm": 0.5619393587112427,
      "learning_rate": 8.958376217373428e-05,
      "loss": 0.0689,
      "step": 960
    },
    {
      "epoch": 0.8734804142278253,
      "grad_norm": 1.0216476917266846,
      "learning_rate": 8.926138451810415e-05,
      "loss": 0.0718,
      "step": 970
    },
    {
      "epoch": 0.8824853669518236,
      "grad_norm": 1.0595803260803223,
      "learning_rate": 8.893469278546491e-05,
      "loss": 0.0482,
      "step": 980
    },
    {
      "epoch": 0.8914903196758217,
      "grad_norm": 0.8070069551467896,
      "learning_rate": 8.860372287300431e-05,
      "loss": 0.0694,
      "step": 990
    },
    {
      "epoch": 0.9004952723998199,
      "grad_norm": 0.9602346420288086,
      "learning_rate": 8.82685111480005e-05,
      "loss": 0.0677,
      "step": 1000
    },
    {
      "epoch": 0.9095002251238181,
      "grad_norm": 0.8625622391700745,
      "learning_rate": 8.792909444382584e-05,
      "loss": 0.0563,
      "step": 1010
    },
    {
      "epoch": 0.9185051778478163,
      "grad_norm": 0.4401424527168274,
      "learning_rate": 8.758551005589967e-05,
      "loss": 0.078,
      "step": 1020
    },
    {
      "epoch": 0.9275101305718145,
      "grad_norm": 0.5410646200180054,
      "learning_rate": 8.723779573759027e-05,
      "loss": 0.066,
      "step": 1030
    },
    {
      "epoch": 0.9365150832958127,
      "grad_norm": 0.40159350633621216,
      "learning_rate": 8.68859896960665e-05,
      "loss": 0.0668,
      "step": 1040
    },
    {
      "epoch": 0.9455200360198109,
      "grad_norm": 0.42848828434944153,
      "learning_rate": 8.653013058809944e-05,
      "loss": 0.0821,
      "step": 1050
    },
    {
      "epoch": 0.9545249887438091,
      "grad_norm": 1.4636023044586182,
      "learning_rate": 8.617025751581489e-05,
      "loss": 0.0713,
      "step": 1060
    },
    {
      "epoch": 0.9635299414678073,
      "grad_norm": 0.8830696940422058,
      "learning_rate": 8.580641002239676e-05,
      "loss": 0.0749,
      "step": 1070
    },
    {
      "epoch": 0.9725348941918055,
      "grad_norm": 0.7653146982192993,
      "learning_rate": 8.543862808774192e-05,
      "loss": 0.0567,
      "step": 1080
    },
    {
      "epoch": 0.9815398469158036,
      "grad_norm": 0.7705882787704468,
      "learning_rate": 8.506695212406735e-05,
      "loss": 0.0574,
      "step": 1090
    },
    {
      "epoch": 0.9905447996398019,
      "grad_norm": 1.1850725412368774,
      "learning_rate": 8.469142297146949e-05,
      "loss": 0.0489,
      "step": 1100
    },
    {
      "epoch": 0.9995497523638001,
      "grad_norm": 2.0228159427642822,
      "learning_rate": 8.43120818934367e-05,
      "loss": 0.0787,
      "step": 1110
    },
    {
      "epoch": 0.9995497523638001,
      "eval_loss": 0.1159963607788086,
      "eval_runtime": 255.9274,
      "eval_samples_per_second": 4.876,
      "eval_steps_per_second": 1.219,
      "step": 1110
    },
    {
      "epoch": 1.0085547050877983,
      "grad_norm": 0.9228658676147461,
      "learning_rate": 8.392897057231519e-05,
      "loss": 0.0504,
      "step": 1120
    },
    {
      "epoch": 1.0175596578117965,
      "grad_norm": 0.5944135189056396,
      "learning_rate": 8.354213110472901e-05,
      "loss": 0.0475,
      "step": 1130
    },
    {
      "epoch": 1.0265646105357946,
      "grad_norm": 0.7088311910629272,
      "learning_rate": 8.315160599695434e-05,
      "loss": 0.0372,
      "step": 1140
    },
    {
      "epoch": 1.035569563259793,
      "grad_norm": 0.8436554670333862,
      "learning_rate": 8.275743816024885e-05,
      "loss": 0.0564,
      "step": 1150
    },
    {
      "epoch": 1.0445745159837911,
      "grad_norm": 0.4579513370990753,
      "learning_rate": 8.23596709061367e-05,
      "loss": 0.0431,
      "step": 1160
    },
    {
      "epoch": 1.0535794687077893,
      "grad_norm": 0.9511253833770752,
      "learning_rate": 8.195834794164925e-05,
      "loss": 0.0365,
      "step": 1170
    },
    {
      "epoch": 1.0625844214317874,
      "grad_norm": 0.8387937545776367,
      "learning_rate": 8.155351336452264e-05,
      "loss": 0.0377,
      "step": 1180
    },
    {
      "epoch": 1.0715893741557856,
      "grad_norm": 0.9661369323730469,
      "learning_rate": 8.11452116583522e-05,
      "loss": 0.0406,
      "step": 1190
    },
    {
      "epoch": 1.080594326879784,
      "grad_norm": 1.0007058382034302,
      "learning_rate": 8.073348768770462e-05,
      "loss": 0.034,
      "step": 1200
    },
    {
      "epoch": 1.0895992796037821,
      "grad_norm": 1.2522867918014526,
      "learning_rate": 8.031838669318814e-05,
      "loss": 0.0469,
      "step": 1210
    },
    {
      "epoch": 1.0986042323277803,
      "grad_norm": 0.5192747712135315,
      "learning_rate": 7.989995428648148e-05,
      "loss": 0.0385,
      "step": 1220
    },
    {
      "epoch": 1.1076091850517784,
      "grad_norm": 0.3283131420612335,
      "learning_rate": 7.947823644532198e-05,
      "loss": 0.0483,
      "step": 1230
    },
    {
      "epoch": 1.1166141377757768,
      "grad_norm": 0.8290643692016602,
      "learning_rate": 7.905327950845358e-05,
      "loss": 0.0264,
      "step": 1240
    },
    {
      "epoch": 1.125619090499775,
      "grad_norm": 0.40350648760795593,
      "learning_rate": 7.862513017053498e-05,
      "loss": 0.0473,
      "step": 1250
    },
    {
      "epoch": 1.134624043223773,
      "grad_norm": 1.0602108240127563,
      "learning_rate": 7.81938354770089e-05,
      "loss": 0.0323,
      "step": 1260
    },
    {
      "epoch": 1.1436289959477712,
      "grad_norm": 0.26863527297973633,
      "learning_rate": 7.775944281893258e-05,
      "loss": 0.0326,
      "step": 1270
    },
    {
      "epoch": 1.1526339486717694,
      "grad_norm": 0.9555085897445679,
      "learning_rate": 7.732199992777045e-05,
      "loss": 0.0314,
      "step": 1280
    },
    {
      "epoch": 1.1616389013957678,
      "grad_norm": 0.8238455057144165,
      "learning_rate": 7.688155487014936e-05,
      "loss": 0.0356,
      "step": 1290
    },
    {
      "epoch": 1.170643854119766,
      "grad_norm": 0.8153785467147827,
      "learning_rate": 7.643815604257703e-05,
      "loss": 0.0292,
      "step": 1300
    },
    {
      "epoch": 1.179648806843764,
      "grad_norm": 0.4006539285182953,
      "learning_rate": 7.599185216612403e-05,
      "loss": 0.0502,
      "step": 1310
    },
    {
      "epoch": 1.1886537595677622,
      "grad_norm": 0.9229021072387695,
      "learning_rate": 7.554269228107044e-05,
      "loss": 0.0477,
      "step": 1320
    },
    {
      "epoch": 1.1976587122917604,
      "grad_norm": 1.1115052700042725,
      "learning_rate": 7.509072574151719e-05,
      "loss": 0.0336,
      "step": 1330
    },
    {
      "epoch": 1.2066636650157587,
      "grad_norm": 1.1647509336471558,
      "learning_rate": 7.463600220996295e-05,
      "loss": 0.0446,
      "step": 1340
    },
    {
      "epoch": 1.2156686177397569,
      "grad_norm": 1.006868839263916,
      "learning_rate": 7.417857165184723e-05,
      "loss": 0.0411,
      "step": 1350
    },
    {
      "epoch": 1.224673570463755,
      "grad_norm": 0.6894146800041199,
      "learning_rate": 7.37184843300601e-05,
      "loss": 0.0338,
      "step": 1360
    },
    {
      "epoch": 1.2336785231877532,
      "grad_norm": 1.1302410364151,
      "learning_rate": 7.325579079941928e-05,
      "loss": 0.0392,
      "step": 1370
    },
    {
      "epoch": 1.2426834759117515,
      "grad_norm": 0.5486178398132324,
      "learning_rate": 7.279054190111506e-05,
      "loss": 0.0424,
      "step": 1380
    },
    {
      "epoch": 1.2516884286357497,
      "grad_norm": 0.1352510303258896,
      "learning_rate": 7.232278875712396e-05,
      "loss": 0.035,
      "step": 1390
    },
    {
      "epoch": 1.2606933813597478,
      "grad_norm": 0.9406415224075317,
      "learning_rate": 7.185258276459125e-05,
      "loss": 0.0419,
      "step": 1400
    },
    {
      "epoch": 1.269698334083746,
      "grad_norm": 0.8745960593223572,
      "learning_rate": 7.137997559018346e-05,
      "loss": 0.0376,
      "step": 1410
    },
    {
      "epoch": 1.2787032868077444,
      "grad_norm": 0.1998104453086853,
      "learning_rate": 7.090501916441124e-05,
      "loss": 0.0335,
      "step": 1420
    },
    {
      "epoch": 1.2877082395317425,
      "grad_norm": 0.30626386404037476,
      "learning_rate": 7.042776567592305e-05,
      "loss": 0.0236,
      "step": 1430
    },
    {
      "epoch": 1.2967131922557407,
      "grad_norm": 0.5227240324020386,
      "learning_rate": 6.994826756577082e-05,
      "loss": 0.037,
      "step": 1440
    },
    {
      "epoch": 1.3057181449797388,
      "grad_norm": 1.066407561302185,
      "learning_rate": 6.946657752164749e-05,
      "loss": 0.0288,
      "step": 1450
    },
    {
      "epoch": 1.314723097703737,
      "grad_norm": 0.5176804065704346,
      "learning_rate": 6.898274847209775e-05,
      "loss": 0.0359,
      "step": 1460
    },
    {
      "epoch": 1.3237280504277353,
      "grad_norm": 0.78618985414505,
      "learning_rate": 6.849683358070217e-05,
      "loss": 0.0329,
      "step": 1470
    },
    {
      "epoch": 1.3327330031517335,
      "grad_norm": 0.5272274017333984,
      "learning_rate": 6.800888624023553e-05,
      "loss": 0.0215,
      "step": 1480
    },
    {
      "epoch": 1.3417379558757316,
      "grad_norm": 0.3429282605648041,
      "learning_rate": 6.751896006679999e-05,
      "loss": 0.0442,
      "step": 1490
    },
    {
      "epoch": 1.3507429085997298,
      "grad_norm": 0.3798148036003113,
      "learning_rate": 6.702710889393369e-05,
      "loss": 0.0423,
      "step": 1500
    },
    {
      "epoch": 1.359747861323728,
      "grad_norm": 0.16493941843509674,
      "learning_rate": 6.653338676669549e-05,
      "loss": 0.0327,
      "step": 1510
    },
    {
      "epoch": 1.3687528140477263,
      "grad_norm": 1.1861361265182495,
      "learning_rate": 6.60378479357264e-05,
      "loss": 0.0342,
      "step": 1520
    },
    {
      "epoch": 1.3777577667717245,
      "grad_norm": 0.9815464615821838,
      "learning_rate": 6.554054685128856e-05,
      "loss": 0.0354,
      "step": 1530
    },
    {
      "epoch": 1.3867627194957226,
      "grad_norm": 1.3809399604797363,
      "learning_rate": 6.50415381572821e-05,
      "loss": 0.0313,
      "step": 1540
    },
    {
      "epoch": 1.395767672219721,
      "grad_norm": 0.9306660890579224,
      "learning_rate": 6.454087668524087e-05,
      "loss": 0.0218,
      "step": 1550
    },
    {
      "epoch": 1.4047726249437191,
      "grad_norm": 2.467954158782959,
      "learning_rate": 6.403861744830747e-05,
      "loss": 0.0294,
      "step": 1560
    },
    {
      "epoch": 1.4137775776677173,
      "grad_norm": 0.5156991481781006,
      "learning_rate": 6.353481563518842e-05,
      "loss": 0.0384,
      "step": 1570
    },
    {
      "epoch": 1.4227825303917154,
      "grad_norm": 1.6558866500854492,
      "learning_rate": 6.302952660408988e-05,
      "loss": 0.0444,
      "step": 1580
    },
    {
      "epoch": 1.4317874831157136,
      "grad_norm": 0.654212474822998,
      "learning_rate": 6.252280587663492e-05,
      "loss": 0.0392,
      "step": 1590
    },
    {
      "epoch": 1.440792435839712,
      "grad_norm": 0.6439039707183838,
      "learning_rate": 6.201470913176273e-05,
      "loss": 0.0244,
      "step": 1600
    },
    {
      "epoch": 1.44979738856371,
      "grad_norm": 0.8767172694206238,
      "learning_rate": 6.150529219961051e-05,
      "loss": 0.0345,
      "step": 1610
    },
    {
      "epoch": 1.4588023412877082,
      "grad_norm": 1.3260753154754639,
      "learning_rate": 6.099461105537889e-05,
      "loss": 0.0298,
      "step": 1620
    },
    {
      "epoch": 1.4678072940117064,
      "grad_norm": 1.4836244583129883,
      "learning_rate": 6.0482721813181276e-05,
      "loss": 0.0269,
      "step": 1630
    },
    {
      "epoch": 1.4768122467357045,
      "grad_norm": 0.5806770920753479,
      "learning_rate": 5.9969680719877996e-05,
      "loss": 0.0319,
      "step": 1640
    },
    {
      "epoch": 1.485817199459703,
      "grad_norm": 1.3763208389282227,
      "learning_rate": 5.945554414889582e-05,
      "loss": 0.0476,
      "step": 1650
    },
    {
      "epoch": 1.494822152183701,
      "grad_norm": 0.9301358461380005,
      "learning_rate": 5.8940368594033626e-05,
      "loss": 0.0342,
      "step": 1660
    },
    {
      "epoch": 1.5038271049076992,
      "grad_norm": 0.6689479947090149,
      "learning_rate": 5.8424210663254787e-05,
      "loss": 0.0266,
      "step": 1670
    },
    {
      "epoch": 1.5128320576316976,
      "grad_norm": 0.530071496963501,
      "learning_rate": 5.790712707246705e-05,
      "loss": 0.0333,
      "step": 1680
    },
    {
      "epoch": 1.5218370103556955,
      "grad_norm": 2.2693254947662354,
      "learning_rate": 5.738917463929052e-05,
      "loss": 0.0279,
      "step": 1690
    },
    {
      "epoch": 1.530841963079694,
      "grad_norm": 1.0499690771102905,
      "learning_rate": 5.6870410276814546e-05,
      "loss": 0.0237,
      "step": 1700
    },
    {
      "epoch": 1.539846915803692,
      "grad_norm": 1.0106838941574097,
      "learning_rate": 5.6350890987343944e-05,
      "loss": 0.0367,
      "step": 1710
    },
    {
      "epoch": 1.5488518685276902,
      "grad_norm": 0.09557852149009705,
      "learning_rate": 5.583067385613565e-05,
      "loss": 0.0274,
      "step": 1720
    },
    {
      "epoch": 1.5578568212516886,
      "grad_norm": 0.575991690158844,
      "learning_rate": 5.5309816045126115e-05,
      "loss": 0.0219,
      "step": 1730
    },
    {
      "epoch": 1.5668617739756865,
      "grad_norm": 0.21184846758842468,
      "learning_rate": 5.478837478665021e-05,
      "loss": 0.0241,
      "step": 1740
    },
    {
      "epoch": 1.5758667266996849,
      "grad_norm": 0.167710080742836,
      "learning_rate": 5.4266407377152586e-05,
      "loss": 0.0246,
      "step": 1750
    },
    {
      "epoch": 1.584871679423683,
      "grad_norm": 0.09416590631008148,
      "learning_rate": 5.374397117089185e-05,
      "loss": 0.0298,
      "step": 1760
    },
    {
      "epoch": 1.5938766321476812,
      "grad_norm": 0.8044834136962891,
      "learning_rate": 5.322112357363841e-05,
      "loss": 0.0499,
      "step": 1770
    },
    {
      "epoch": 1.6028815848716795,
      "grad_norm": 0.6561732888221741,
      "learning_rate": 5.2697922036366746e-05,
      "loss": 0.022,
      "step": 1780
    },
    {
      "epoch": 1.6118865375956775,
      "grad_norm": 1.3047144412994385,
      "learning_rate": 5.217442404894254e-05,
      "loss": 0.0302,
      "step": 1790
    },
    {
      "epoch": 1.6208914903196758,
      "grad_norm": 0.3679428696632385,
      "learning_rate": 5.1650687133805674e-05,
      "loss": 0.0234,
      "step": 1800
    },
    {
      "epoch": 1.629896443043674,
      "grad_norm": 0.2505779266357422,
      "learning_rate": 5.112676883964972e-05,
      "loss": 0.0259,
      "step": 1810
    },
    {
      "epoch": 1.6389013957676721,
      "grad_norm": 0.22162729501724243,
      "learning_rate": 5.0602726735098235e-05,
      "loss": 0.0315,
      "step": 1820
    },
    {
      "epoch": 1.6479063484916705,
      "grad_norm": 0.8099084496498108,
      "learning_rate": 5.007861840237923e-05,
      "loss": 0.0258,
      "step": 1830
    },
    {
      "epoch": 1.6569113012156687,
      "grad_norm": 0.2524292767047882,
      "learning_rate": 4.9554501430997934e-05,
      "loss": 0.0329,
      "step": 1840
    },
    {
      "epoch": 1.6659162539396668,
      "grad_norm": 0.6876524090766907,
      "learning_rate": 4.903043341140879e-05,
      "loss": 0.0263,
      "step": 1850
    },
    {
      "epoch": 1.6749212066636652,
      "grad_norm": 1.4806251525878906,
      "learning_rate": 4.850647192868735e-05,
      "loss": 0.0334,
      "step": 1860
    },
    {
      "epoch": 1.683926159387663,
      "grad_norm": 0.5879294872283936,
      "learning_rate": 4.798267455620283e-05,
      "loss": 0.0144,
      "step": 1870
    },
    {
      "epoch": 1.6929311121116615,
      "grad_norm": 0.8280892372131348,
      "learning_rate": 4.745909884929183e-05,
      "loss": 0.021,
      "step": 1880
    },
    {
      "epoch": 1.7019360648356596,
      "grad_norm": 0.11405783146619797,
      "learning_rate": 4.69358023389342e-05,
      "loss": 0.0248,
      "step": 1890
    },
    {
      "epoch": 1.7109410175596578,
      "grad_norm": 0.7534868717193604,
      "learning_rate": 4.641284252543131e-05,
      "loss": 0.0288,
      "step": 1900
    },
    {
      "epoch": 1.7199459702836561,
      "grad_norm": 1.2606899738311768,
      "learning_rate": 4.589027687208806e-05,
      "loss": 0.0152,
      "step": 1910
    },
    {
      "epoch": 1.728950923007654,
      "grad_norm": 0.7043948769569397,
      "learning_rate": 4.536816279889865e-05,
      "loss": 0.0459,
      "step": 1920
    },
    {
      "epoch": 1.7379558757316524,
      "grad_norm": 0.48113909363746643,
      "learning_rate": 4.484655767623719e-05,
      "loss": 0.0297,
      "step": 1930
    },
    {
      "epoch": 1.7469608284556506,
      "grad_norm": 9.093439102172852,
      "learning_rate": 4.432551881855389e-05,
      "loss": 0.0311,
      "step": 1940
    },
    {
      "epoch": 1.7559657811796487,
      "grad_norm": 0.4598866403102875,
      "learning_rate": 4.380510347807725e-05,
      "loss": 0.0378,
      "step": 1950
    },
    {
      "epoch": 1.7649707339036471,
      "grad_norm": 1.7693955898284912,
      "learning_rate": 4.3285368838523075e-05,
      "loss": 0.0369,
      "step": 1960
    },
    {
      "epoch": 1.773975686627645,
      "grad_norm": 0.7284817695617676,
      "learning_rate": 4.2766372008811183e-05,
      "loss": 0.0239,
      "step": 1970
    },
    {
      "epoch": 1.7829806393516434,
      "grad_norm": 0.46821558475494385,
      "learning_rate": 4.224817001679011e-05,
      "loss": 0.0138,
      "step": 1980
    },
    {
      "epoch": 1.7919855920756416,
      "grad_norm": 1.2053416967391968,
      "learning_rate": 4.1730819802970967e-05,
      "loss": 0.0336,
      "step": 1990
    },
    {
      "epoch": 1.8009905447996397,
      "grad_norm": 0.8531839847564697,
      "learning_rate": 4.121437821427061e-05,
      "loss": 0.0323,
      "step": 2000
    },
    {
      "epoch": 1.809995497523638,
      "grad_norm": 0.5530725717544556,
      "learning_rate": 4.06989019977654e-05,
      "loss": 0.0139,
      "step": 2010
    },
    {
      "epoch": 1.8190004502476362,
      "grad_norm": 0.2601499855518341,
      "learning_rate": 4.018444779445571e-05,
      "loss": 0.0196,
      "step": 2020
    },
    {
      "epoch": 1.8280054029716344,
      "grad_norm": 0.5376416444778442,
      "learning_rate": 3.96710721330421e-05,
      "loss": 0.0142,
      "step": 2030
    },
    {
      "epoch": 1.8370103556956328,
      "grad_norm": 2.2207207679748535,
      "learning_rate": 3.9158831423714036e-05,
      "loss": 0.0341,
      "step": 2040
    },
    {
      "epoch": 1.8460153084196307,
      "grad_norm": 1.378247857093811,
      "learning_rate": 3.8647781951951375e-05,
      "loss": 0.0277,
      "step": 2050
    },
    {
      "epoch": 1.855020261143629,
      "grad_norm": 0.653751015663147,
      "learning_rate": 3.813797987233965e-05,
      "loss": 0.025,
      "step": 2060
    },
    {
      "epoch": 1.8640252138676272,
      "grad_norm": 0.7411632537841797,
      "learning_rate": 3.762948120239988e-05,
      "loss": 0.0296,
      "step": 2070
    },
    {
      "epoch": 1.8730301665916254,
      "grad_norm": 0.7868918776512146,
      "learning_rate": 3.712234181643317e-05,
      "loss": 0.0262,
      "step": 2080
    },
    {
      "epoch": 1.8820351193156237,
      "grad_norm": 1.1841440200805664,
      "learning_rate": 3.661661743938128e-05,
      "loss": 0.0465,
      "step": 2090
    },
    {
      "epoch": 1.8910400720396217,
      "grad_norm": 0.7971113920211792,
      "learning_rate": 3.6112363640703476e-05,
      "loss": 0.0147,
      "step": 2100
    },
    {
      "epoch": 1.90004502476362,
      "grad_norm": 0.4902219772338867,
      "learning_rate": 3.5609635828270545e-05,
      "loss": 0.0151,
      "step": 2110
    },
    {
      "epoch": 1.9090499774876182,
      "grad_norm": 0.711567759513855,
      "learning_rate": 3.5108489242276455e-05,
      "loss": 0.0343,
      "step": 2120
    },
    {
      "epoch": 1.9180549302116163,
      "grad_norm": 0.21644097566604614,
      "learning_rate": 3.4608978949168614e-05,
      "loss": 0.013,
      "step": 2130
    },
    {
      "epoch": 1.9270598829356147,
      "grad_norm": 1.515451192855835,
      "learning_rate": 3.411115983559705e-05,
      "loss": 0.0195,
      "step": 2140
    },
    {
      "epoch": 1.9360648356596126,
      "grad_norm": 0.05813518166542053,
      "learning_rate": 3.3615086602383396e-05,
      "loss": 0.0134,
      "step": 2150
    },
    {
      "epoch": 1.945069788383611,
      "grad_norm": 0.3869155943393707,
      "learning_rate": 3.312081375851038e-05,
      "loss": 0.0154,
      "step": 2160
    },
    {
      "epoch": 1.9540747411076091,
      "grad_norm": 0.36261293292045593,
      "learning_rate": 3.2628395615132315e-05,
      "loss": 0.0151,
      "step": 2170
    },
    {
      "epoch": 1.9630796938316073,
      "grad_norm": 1.5767346620559692,
      "learning_rate": 3.213788627960725e-05,
      "loss": 0.0163,
      "step": 2180
    },
    {
      "epoch": 1.9720846465556057,
      "grad_norm": 1.1265090703964233,
      "learning_rate": 3.164933964955174e-05,
      "loss": 0.0236,
      "step": 2190
    },
    {
      "epoch": 1.9810895992796038,
      "grad_norm": 1.0226150751113892,
      "learning_rate": 3.116280940691843e-05,
      "loss": 0.0262,
      "step": 2200
    },
    {
      "epoch": 1.990094552003602,
      "grad_norm": 0.17756378650665283,
      "learning_rate": 3.067834901209744e-05,
      "loss": 0.0389,
      "step": 2210
    },
    {
      "epoch": 1.9990995047276003,
      "grad_norm": 1.1844819784164429,
      "learning_rate": 3.019601169804216e-05,
      "loss": 0.0251,
      "step": 2220
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.026615800336003304,
      "eval_runtime": 256.4464,
      "eval_samples_per_second": 4.867,
      "eval_steps_per_second": 1.217,
      "step": 2221
    },
    {
      "epoch": 2.0081044574515983,
      "grad_norm": 0.17430123686790466,
      "learning_rate": 2.9715850464419902e-05,
      "loss": 0.0125,
      "step": 2230
    },
    {
      "epoch": 2.0171094101755966,
      "grad_norm": 0.049519579857587814,
      "learning_rate": 2.9237918071788217e-05,
      "loss": 0.0037,
      "step": 2240
    },
    {
      "epoch": 2.0261143628995946,
      "grad_norm": 0.7575904130935669,
      "learning_rate": 2.876226703579761e-05,
      "loss": 0.0061,
      "step": 2250
    },
    {
      "epoch": 2.035119315623593,
      "grad_norm": 0.14697220921516418,
      "learning_rate": 2.8288949621421012e-05,
      "loss": 0.0058,
      "step": 2260
    },
    {
      "epoch": 2.0441242683475913,
      "grad_norm": 0.12447185069322586,
      "learning_rate": 2.781801783721084e-05,
      "loss": 0.0033,
      "step": 2270
    },
    {
      "epoch": 2.0531292210715892,
      "grad_norm": 0.07805541902780533,
      "learning_rate": 2.7349523429584305e-05,
      "loss": 0.0028,
      "step": 2280
    },
    {
      "epoch": 2.0621341737955876,
      "grad_norm": 0.04105732962489128,
      "learning_rate": 2.6883517877137404e-05,
      "loss": 0.0028,
      "step": 2290
    },
    {
      "epoch": 2.071139126519586,
      "grad_norm": 0.9780999422073364,
      "learning_rate": 2.6420052384988526e-05,
      "loss": 0.0042,
      "step": 2300
    },
    {
      "epoch": 2.080144079243584,
      "grad_norm": 0.048143476247787476,
      "learning_rate": 2.595917787915176e-05,
      "loss": 0.0037,
      "step": 2310
    },
    {
      "epoch": 2.0891490319675823,
      "grad_norm": 0.02444763109087944,
      "learning_rate": 2.5500945000941367e-05,
      "loss": 0.0037,
      "step": 2320
    },
    {
      "epoch": 2.09815398469158,
      "grad_norm": 0.188834547996521,
      "learning_rate": 2.5045404101407076e-05,
      "loss": 0.0059,
      "step": 2330
    },
    {
      "epoch": 2.1071589374155786,
      "grad_norm": 0.02581826224923134,
      "learning_rate": 2.459260523580154e-05,
      "loss": 0.0089,
      "step": 2340
    },
    {
      "epoch": 2.116163890139577,
      "grad_norm": 0.027516735717654228,
      "learning_rate": 2.4142598158080186e-05,
      "loss": 0.0029,
      "step": 2350
    },
    {
      "epoch": 2.125168842863575,
      "grad_norm": 0.31967952847480774,
      "learning_rate": 2.369543231543425e-05,
      "loss": 0.0009,
      "step": 2360
    },
    {
      "epoch": 2.1341737955875733,
      "grad_norm": 0.06609421223402023,
      "learning_rate": 2.325115684285743e-05,
      "loss": 0.0066,
      "step": 2370
    },
    {
      "epoch": 2.143178748311571,
      "grad_norm": 0.7793627381324768,
      "learning_rate": 2.2809820557746887e-05,
      "loss": 0.0091,
      "step": 2380
    },
    {
      "epoch": 2.1521837010355696,
      "grad_norm": 1.8375996351242065,
      "learning_rate": 2.2371471954539234e-05,
      "loss": 0.0037,
      "step": 2390
    },
    {
      "epoch": 2.161188653759568,
      "grad_norm": 0.5395260453224182,
      "learning_rate": 2.1936159199381743e-05,
      "loss": 0.0074,
      "step": 2400
    },
    {
      "epoch": 2.170193606483566,
      "grad_norm": 0.12267076969146729,
      "learning_rate": 2.1503930124840017e-05,
      "loss": 0.007,
      "step": 2410
    },
    {
      "epoch": 2.1791985592075642,
      "grad_norm": 0.4727039337158203,
      "learning_rate": 2.1074832224641934e-05,
      "loss": 0.0047,
      "step": 2420
    },
    {
      "epoch": 2.1882035119315626,
      "grad_norm": 0.052199505269527435,
      "learning_rate": 2.0648912648459074e-05,
      "loss": 0.0054,
      "step": 2430
    },
    {
      "epoch": 2.1972084646555605,
      "grad_norm": 0.1152840107679367,
      "learning_rate": 2.0226218196725867e-05,
      "loss": 0.0015,
      "step": 2440
    },
    {
      "epoch": 2.206213417379559,
      "grad_norm": 0.22289854288101196,
      "learning_rate": 1.980679531549708e-05,
      "loss": 0.0027,
      "step": 2450
    },
    {
      "epoch": 2.215218370103557,
      "grad_norm": 0.36808642745018005,
      "learning_rate": 1.939069009134433e-05,
      "loss": 0.0062,
      "step": 2460
    },
    {
      "epoch": 2.224223322827555,
      "grad_norm": 0.036651868373155594,
      "learning_rate": 1.8977948246292e-05,
      "loss": 0.0024,
      "step": 2470
    },
    {
      "epoch": 2.2332282755515536,
      "grad_norm": 0.49448227882385254,
      "learning_rate": 1.8568615132793355e-05,
      "loss": 0.0032,
      "step": 2480
    },
    {
      "epoch": 2.2422332282755515,
      "grad_norm": 0.2644898295402527,
      "learning_rate": 1.8162735728746975e-05,
      "loss": 0.0046,
      "step": 2490
    },
    {
      "epoch": 2.25123818099955,
      "grad_norm": 0.8714432716369629,
      "learning_rate": 1.776035463255481e-05,
      "loss": 0.0073,
      "step": 2500
    },
    {
      "epoch": 2.260243133723548,
      "grad_norm": 0.010121546685695648,
      "learning_rate": 1.7361516058221443e-05,
      "loss": 0.0038,
      "step": 2510
    },
    {
      "epoch": 2.269248086447546,
      "grad_norm": 0.02512921765446663,
      "learning_rate": 1.6966263830495936e-05,
      "loss": 0.0027,
      "step": 2520
    },
    {
      "epoch": 2.2782530391715445,
      "grad_norm": 0.009355619549751282,
      "learning_rate": 1.6574641380056254e-05,
      "loss": 0.0007,
      "step": 2530
    },
    {
      "epoch": 2.2872579918955425,
      "grad_norm": 0.05226243659853935,
      "learning_rate": 1.6186691738737177e-05,
      "loss": 0.0044,
      "step": 2540
    },
    {
      "epoch": 2.296262944619541,
      "grad_norm": 0.10525865852832794,
      "learning_rate": 1.5802457534801718e-05,
      "loss": 0.0031,
      "step": 2550
    },
    {
      "epoch": 2.3052678973435388,
      "grad_norm": 0.6201657056808472,
      "learning_rate": 1.542198098825734e-05,
      "loss": 0.0059,
      "step": 2560
    },
    {
      "epoch": 2.314272850067537,
      "grad_norm": 0.04984705522656441,
      "learning_rate": 1.5045303906216596e-05,
      "loss": 0.0019,
      "step": 2570
    },
    {
      "epoch": 2.3232778027915355,
      "grad_norm": 0.07082956284284592,
      "learning_rate": 1.4672467678303386e-05,
      "loss": 0.0048,
      "step": 2580
    },
    {
      "epoch": 2.3322827555155334,
      "grad_norm": 0.0620376318693161,
      "learning_rate": 1.4303513272105057e-05,
      "loss": 0.0042,
      "step": 2590
    },
    {
      "epoch": 2.341287708239532,
      "grad_norm": 0.1929488629102707,
      "learning_rate": 1.3938481228670774e-05,
      "loss": 0.0046,
      "step": 2600
    },
    {
      "epoch": 2.3502926609635297,
      "grad_norm": 0.05772895738482475,
      "learning_rate": 1.3577411658056966e-05,
      "loss": 0.0006,
      "step": 2610
    },
    {
      "epoch": 2.359297613687528,
      "grad_norm": 0.08116903901100159,
      "learning_rate": 1.322034423491978e-05,
      "loss": 0.0025,
      "step": 2620
    },
    {
      "epoch": 2.3683025664115265,
      "grad_norm": 0.9051384329795837,
      "learning_rate": 1.286731819415583e-05,
      "loss": 0.0114,
      "step": 2630
    },
    {
      "epoch": 2.3773075191355244,
      "grad_norm": 0.009844019077718258,
      "learning_rate": 1.2518372326590877e-05,
      "loss": 0.002,
      "step": 2640
    },
    {
      "epoch": 2.3863124718595228,
      "grad_norm": 0.294877290725708,
      "learning_rate": 1.2173544974717494e-05,
      "loss": 0.0043,
      "step": 2650
    },
    {
      "epoch": 2.3953174245835207,
      "grad_norm": 0.10471183061599731,
      "learning_rate": 1.1832874028481978e-05,
      "loss": 0.0067,
      "step": 2660
    },
    {
      "epoch": 2.404322377307519,
      "grad_norm": 0.05715145543217659,
      "learning_rate": 1.149639692112095e-05,
      "loss": 0.0021,
      "step": 2670
    },
    {
      "epoch": 2.4133273300315174,
      "grad_norm": 0.07973455637693405,
      "learning_rate": 1.1164150625048164e-05,
      "loss": 0.0077,
      "step": 2680
    },
    {
      "epoch": 2.4223322827555154,
      "grad_norm": 0.2005072385072708,
      "learning_rate": 1.0836171647791937e-05,
      "loss": 0.0036,
      "step": 2690
    },
    {
      "epoch": 2.4313372354795137,
      "grad_norm": 0.03794039785861969,
      "learning_rate": 1.0512496027983714e-05,
      "loss": 0.0014,
      "step": 2700
    },
    {
      "epoch": 2.440342188203512,
      "grad_norm": 0.2521399259567261,
      "learning_rate": 1.0193159331397978e-05,
      "loss": 0.002,
      "step": 2710
    },
    {
      "epoch": 2.44934714092751,
      "grad_norm": 0.5675289034843445,
      "learning_rate": 9.878196647044435e-06,
      "loss": 0.0017,
      "step": 2720
    },
    {
      "epoch": 2.4583520936515084,
      "grad_norm": 0.11974442005157471,
      "learning_rate": 9.56764258331226e-06,
      "loss": 0.0065,
      "step": 2730
    },
    {
      "epoch": 2.4673570463755063,
      "grad_norm": 0.007801082916557789,
      "learning_rate": 9.261531264167344e-06,
      "loss": 0.0036,
      "step": 2740
    },
    {
      "epoch": 2.4763619990995047,
      "grad_norm": 0.10996538400650024,
      "learning_rate": 8.959896325402727e-06,
      "loss": 0.0069,
      "step": 2750
    },
    {
      "epoch": 2.485366951823503,
      "grad_norm": 0.10477505624294281,
      "learning_rate": 8.662770910942692e-06,
      "loss": 0.0081,
      "step": 2760
    },
    {
      "epoch": 2.494371904547501,
      "grad_norm": 0.2026921659708023,
      "learning_rate": 8.370187669200763e-06,
      "loss": 0.0036,
      "step": 2770
    },
    {
      "epoch": 2.5033768572714994,
      "grad_norm": 0.021472468972206116,
      "learning_rate": 8.082178749492447e-06,
      "loss": 0.0048,
      "step": 2780
    },
    {
      "epoch": 2.5123818099954978,
      "grad_norm": 0.9304440021514893,
      "learning_rate": 7.798775798502483e-06,
      "loss": 0.005,
      "step": 2790
    },
    {
      "epoch": 2.5213867627194957,
      "grad_norm": 0.020146839320659637,
      "learning_rate": 7.520009956807561e-06,
      "loss": 0.0008,
      "step": 2800
    },
    {
      "epoch": 2.530391715443494,
      "grad_norm": 0.6632398962974548,
      "learning_rate": 7.245911855454524e-06,
      "loss": 0.0067,
      "step": 2810
    },
    {
      "epoch": 2.539396668167492,
      "grad_norm": 0.15244323015213013,
      "learning_rate": 6.976511612594622e-06,
      "loss": 0.0014,
      "step": 2820
    },
    {
      "epoch": 2.5484016208914904,
      "grad_norm": 0.062212031334638596,
      "learning_rate": 6.711838830174105e-06,
      "loss": 0.0105,
      "step": 2830
    },
    {
      "epoch": 2.5574065736154887,
      "grad_norm": 0.3469594120979309,
      "learning_rate": 6.451922590681508e-06,
      "loss": 0.0031,
      "step": 2840
    },
    {
      "epoch": 2.5664115263394867,
      "grad_norm": 0.03760126233100891,
      "learning_rate": 6.1967914539521e-06,
      "loss": 0.0025,
      "step": 2850
    },
    {
      "epoch": 2.575416479063485,
      "grad_norm": 0.033643998205661774,
      "learning_rate": 5.946473454029594e-06,
      "loss": 0.0038,
      "step": 2860
    },
    {
      "epoch": 2.584421431787483,
      "grad_norm": 0.0025672377087175846,
      "learning_rate": 5.70099609608587e-06,
      "loss": 0.0023,
      "step": 2870
    },
    {
      "epoch": 2.5934263845114813,
      "grad_norm": 0.05400478467345238,
      "learning_rate": 5.460386353398583e-06,
      "loss": 0.002,
      "step": 2880
    },
    {
      "epoch": 2.6024313372354797,
      "grad_norm": 0.12372594326734543,
      "learning_rate": 5.2246706643873715e-06,
      "loss": 0.003,
      "step": 2890
    },
    {
      "epoch": 2.6114362899594776,
      "grad_norm": 0.004354070406407118,
      "learning_rate": 4.993874929708742e-06,
      "loss": 0.0054,
      "step": 2900
    },
    {
      "epoch": 2.620441242683476,
      "grad_norm": 0.06157950684428215,
      "learning_rate": 4.768024509410096e-06,
      "loss": 0.0053,
      "step": 2910
    },
    {
      "epoch": 2.629446195407474,
      "grad_norm": 0.243352010846138,
      "learning_rate": 4.547144220143185e-06,
      "loss": 0.0011,
      "step": 2920
    },
    {
      "epoch": 2.6384511481314723,
      "grad_norm": 0.016507180407643318,
      "learning_rate": 4.331258332437127e-06,
      "loss": 0.0032,
      "step": 2930
    },
    {
      "epoch": 2.6474561008554707,
      "grad_norm": 0.1703907698392868,
      "learning_rate": 4.120390568031673e-06,
      "loss": 0.0029,
      "step": 2940
    },
    {
      "epoch": 2.6564610535794686,
      "grad_norm": 0.07018712908029556,
      "learning_rate": 3.914564097270545e-06,
      "loss": 0.0007,
      "step": 2950
    },
    {
      "epoch": 2.665466006303467,
      "grad_norm": 0.12229815125465393,
      "learning_rate": 3.7138015365554833e-06,
      "loss": 0.0034,
      "step": 2960
    },
    {
      "epoch": 2.674470959027465,
      "grad_norm": 2.5113251209259033,
      "learning_rate": 3.51812494586114e-06,
      "loss": 0.0041,
      "step": 2970
    },
    {
      "epoch": 2.6834759117514633,
      "grad_norm": 0.0726880431175232,
      "learning_rate": 3.327555826311135e-06,
      "loss": 0.0027,
      "step": 2980
    },
    {
      "epoch": 2.6924808644754616,
      "grad_norm": 0.03436645492911339,
      "learning_rate": 3.142115117815414e-06,
      "loss": 0.0065,
      "step": 2990
    },
    {
      "epoch": 2.7014858171994596,
      "grad_norm": 0.4140453636646271,
      "learning_rate": 2.9618231967694532e-06,
      "loss": 0.0007,
      "step": 3000
    },
    {
      "epoch": 2.710490769923458,
      "grad_norm": 0.006208219565451145,
      "learning_rate": 2.7866998738152017e-06,
      "loss": 0.0012,
      "step": 3010
    },
    {
      "epoch": 2.719495722647456,
      "grad_norm": 0.4179028570652008,
      "learning_rate": 2.616764391664317e-06,
      "loss": 0.0018,
      "step": 3020
    },
    {
      "epoch": 2.7285006753714542,
      "grad_norm": 0.002532988553866744,
      "learning_rate": 2.452035422983734e-06,
      "loss": 0.0054,
      "step": 3030
    },
    {
      "epoch": 2.7375056280954526,
      "grad_norm": 0.023051414638757706,
      "learning_rate": 2.292531068343906e-06,
      "loss": 0.0065,
      "step": 3040
    },
    {
      "epoch": 2.7465105808194505,
      "grad_norm": 0.0872783362865448,
      "learning_rate": 2.1382688542298913e-06,
      "loss": 0.0009,
      "step": 3050
    },
    {
      "epoch": 2.755515533543449,
      "grad_norm": 0.07450630515813828,
      "learning_rate": 1.9892657311155248e-06,
      "loss": 0.0016,
      "step": 3060
    },
    {
      "epoch": 2.764520486267447,
      "grad_norm": 0.7807486057281494,
      "learning_rate": 1.8455380716009164e-06,
      "loss": 0.0034,
      "step": 3070
    },
    {
      "epoch": 2.773525438991445,
      "grad_norm": 0.1835281103849411,
      "learning_rate": 1.7071016686133478e-06,
      "loss": 0.0023,
      "step": 3080
    },
    {
      "epoch": 2.7825303917154436,
      "grad_norm": 0.009540006518363953,
      "learning_rate": 1.5739717336720084e-06,
      "loss": 0.004,
      "step": 3090
    },
    {
      "epoch": 2.791535344439442,
      "grad_norm": 0.07883923500776291,
      "learning_rate": 1.446162895216474e-06,
      "loss": 0.0045,
      "step": 3100
    },
    {
      "epoch": 2.80054029716344,
      "grad_norm": 0.04028301313519478,
      "learning_rate": 1.3236891969993726e-06,
      "loss": 0.0068,
      "step": 3110
    },
    {
      "epoch": 2.8095452498874383,
      "grad_norm": 0.021916726604104042,
      "learning_rate": 1.2065640965432001e-06,
      "loss": 0.0009,
      "step": 3120
    },
    {
      "epoch": 2.818550202611436,
      "grad_norm": 0.03834931552410126,
      "learning_rate": 1.0948004636616216e-06,
      "loss": 0.0007,
      "step": 3130
    },
    {
      "epoch": 2.8275551553354346,
      "grad_norm": 0.21462590992450714,
      "learning_rate": 9.884105790453235e-07,
      "loss": 0.0136,
      "step": 3140
    },
    {
      "epoch": 2.836560108059433,
      "grad_norm": 0.022002585232257843,
      "learning_rate": 8.874061329125938e-07,
      "loss": 0.0055,
      "step": 3150
    },
    {
      "epoch": 2.845565060783431,
      "grad_norm": 0.9805160164833069,
      "learning_rate": 7.917982237247934e-07,
      "loss": 0.004,
      "step": 3160
    },
    {
      "epoch": 2.8545700135074292,
      "grad_norm": 0.2588270306587219,
      "learning_rate": 7.015973569668321e-07,
      "loss": 0.0026,
      "step": 3170
    },
    {
      "epoch": 2.863574966231427,
      "grad_norm": 0.10917803645133972,
      "learning_rate": 6.168134439928364e-07,
      "loss": 0.0102,
      "step": 3180
    },
    {
      "epoch": 2.8725799189554255,
      "grad_norm": 0.04140583053231239,
      "learning_rate": 5.374558009370811e-07,
      "loss": 0.0022,
      "step": 3190
    },
    {
      "epoch": 2.881584871679424,
      "grad_norm": 0.10920015722513199,
      "learning_rate": 4.635331476903093e-07,
      "loss": 0.0055,
      "step": 3200
    },
    {
      "epoch": 2.890589824403422,
      "grad_norm": 0.7428072690963745,
      "learning_rate": 3.950536069415756e-07,
      "loss": 0.003,
      "step": 3210
    },
    {
      "epoch": 2.89959477712742,
      "grad_norm": 1.1790095567703247,
      "learning_rate": 3.3202470328576043e-07,
      "loss": 0.0041,
      "step": 3220
    },
    {
      "epoch": 2.908599729851418,
      "grad_norm": 0.035397857427597046,
      "learning_rate": 2.744533623967149e-07,
      "loss": 0.0008,
      "step": 3230
    },
    {
      "epoch": 2.9176046825754165,
      "grad_norm": 0.2914693355560303,
      "learning_rate": 2.223459102662695e-07,
      "loss": 0.0103,
      "step": 3240
    },
    {
      "epoch": 2.926609635299415,
      "grad_norm": 0.8179630041122437,
      "learning_rate": 1.7570807250915645e-07,
      "loss": 0.0021,
      "step": 3250
    },
    {
      "epoch": 2.935614588023413,
      "grad_norm": 0.05499008670449257,
      "learning_rate": 1.3454497373384113e-07,
      "loss": 0.0031,
      "step": 3260
    },
    {
      "epoch": 2.944619540747411,
      "grad_norm": 0.03067617118358612,
      "learning_rate": 9.886113697944454e-08,
      "loss": 0.0034,
      "step": 3270
    },
    {
      "epoch": 2.953624493471409,
      "grad_norm": 0.006165851838886738,
      "learning_rate": 6.866048321873542e-08,
      "loss": 0.0007,
      "step": 3280
    },
    {
      "epoch": 2.9626294461954075,
      "grad_norm": 0.09968739748001099,
      "learning_rate": 4.394633092730804e-08,
      "loss": 0.002,
      "step": 3290
    },
    {
      "epoch": 2.971634398919406,
      "grad_norm": 0.2652704417705536,
      "learning_rate": 2.4721395718924068e-08,
      "loss": 0.0025,
      "step": 3300
    },
    {
      "epoch": 2.9806393516434038,
      "grad_norm": 0.01521710678935051,
      "learning_rate": 1.098779004712891e-08,
      "loss": 0.0017,
      "step": 3310
    },
    {
      "epoch": 2.989644304367402,
      "grad_norm": 0.010372631251811981,
      "learning_rate": 2.7470229731318966e-09,
      "loss": 0.0033,
      "step": 3320
    },
    {
      "epoch": 2.9986492570914,
      "grad_norm": 0.39241182804107666,
      "learning_rate": 0.0,
      "loss": 0.0096,
      "step": 3330
    },
    {
      "epoch": 2.9986492570914,
      "eval_loss": 0.03076469525694847,
      "eval_runtime": 256.718,
      "eval_samples_per_second": 4.861,
      "eval_steps_per_second": 1.215,
      "step": 3330
    },
    {
      "epoch": 2.9986492570914,
      "step": 3330,
      "total_flos": 1.0462291960935743e+19,
      "train_loss": 0.08977636006778224,
      "train_runtime": 53756.5351,
      "train_samples_per_second": 1.983,
      "train_steps_per_second": 0.062
    }
  ],
  "logging_steps": 10,
  "max_steps": 3330,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.0462291960935743e+19,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
