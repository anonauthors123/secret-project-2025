{
  "best_metric": 0.02080639824271202,
  "best_model_checkpoint": "saves/source_code_mix_pseudo_code/llm4decompile-9b-v2/lora_sft/checkpoint-3330",
  "epoch": 2.9986492570914,
  "eval_steps": 500,
  "global_step": 3330,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.009004952723998198,
      "grad_norm": 50.080081939697266,
      "learning_rate": 1.5015015015015015e-05,
      "loss": 8.4434,
      "step": 10
    },
    {
      "epoch": 0.018009905447996397,
      "grad_norm": 33.82097244262695,
      "learning_rate": 3.003003003003003e-05,
      "loss": 6.1543,
      "step": 20
    },
    {
      "epoch": 0.0270148581719946,
      "grad_norm": 5.6015706062316895,
      "learning_rate": 4.5045045045045046e-05,
      "loss": 2.3728,
      "step": 30
    },
    {
      "epoch": 0.03601981089599279,
      "grad_norm": 3.8996126651763916,
      "learning_rate": 6.006006006006006e-05,
      "loss": 1.5208,
      "step": 40
    },
    {
      "epoch": 0.045024763619991,
      "grad_norm": 3.73718523979187,
      "learning_rate": 7.507507507507507e-05,
      "loss": 0.6685,
      "step": 50
    },
    {
      "epoch": 0.0540297163439892,
      "grad_norm": 1.1650291681289673,
      "learning_rate": 9.009009009009009e-05,
      "loss": 0.2541,
      "step": 60
    },
    {
      "epoch": 0.0630346690679874,
      "grad_norm": 2.826418876647949,
      "learning_rate": 0.00010510510510510511,
      "loss": 0.2563,
      "step": 70
    },
    {
      "epoch": 0.07203962179198559,
      "grad_norm": 0.7548753619194031,
      "learning_rate": 0.00012012012012012012,
      "loss": 0.221,
      "step": 80
    },
    {
      "epoch": 0.08104457451598379,
      "grad_norm": 2.2989988327026367,
      "learning_rate": 0.00013513513513513514,
      "loss": 0.2153,
      "step": 90
    },
    {
      "epoch": 0.090049527239982,
      "grad_norm": 2.218259572982788,
      "learning_rate": 0.00015015015015015014,
      "loss": 0.2114,
      "step": 100
    },
    {
      "epoch": 0.09905447996398019,
      "grad_norm": 0.7496749758720398,
      "learning_rate": 0.00016516516516516518,
      "loss": 0.1954,
      "step": 110
    },
    {
      "epoch": 0.1080594326879784,
      "grad_norm": 1.5233099460601807,
      "learning_rate": 0.00018018018018018018,
      "loss": 0.1825,
      "step": 120
    },
    {
      "epoch": 0.11706438541197658,
      "grad_norm": 0.8370073437690735,
      "learning_rate": 0.0001951951951951952,
      "loss": 0.1912,
      "step": 130
    },
    {
      "epoch": 0.1260693381359748,
      "grad_norm": 0.5283665657043457,
      "learning_rate": 0.00021021021021021022,
      "loss": 0.181,
      "step": 140
    },
    {
      "epoch": 0.135074290859973,
      "grad_norm": 1.401171088218689,
      "learning_rate": 0.00022522522522522523,
      "loss": 0.1923,
      "step": 150
    },
    {
      "epoch": 0.14407924358397117,
      "grad_norm": 1.3003677129745483,
      "learning_rate": 0.00024024024024024023,
      "loss": 0.157,
      "step": 160
    },
    {
      "epoch": 0.15308419630796938,
      "grad_norm": 0.6860900521278381,
      "learning_rate": 0.00025525525525525527,
      "loss": 0.1404,
      "step": 170
    },
    {
      "epoch": 0.16208914903196758,
      "grad_norm": 0.4879833161830902,
      "learning_rate": 0.0002702702702702703,
      "loss": 0.1486,
      "step": 180
    },
    {
      "epoch": 0.1710941017559658,
      "grad_norm": 0.7550235986709595,
      "learning_rate": 0.0002852852852852853,
      "loss": 0.1303,
      "step": 190
    },
    {
      "epoch": 0.180099054479964,
      "grad_norm": 0.649842381477356,
      "learning_rate": 0.0003003003003003003,
      "loss": 0.2146,
      "step": 200
    },
    {
      "epoch": 0.18910400720396217,
      "grad_norm": 0.38673916459083557,
      "learning_rate": 0.00031531531531531535,
      "loss": 0.1457,
      "step": 210
    },
    {
      "epoch": 0.19810895992796038,
      "grad_norm": 0.9003312587738037,
      "learning_rate": 0.00033033033033033035,
      "loss": 0.1332,
      "step": 220
    },
    {
      "epoch": 0.20711391265195858,
      "grad_norm": 0.32849687337875366,
      "learning_rate": 0.00034534534534534536,
      "loss": 0.1404,
      "step": 230
    },
    {
      "epoch": 0.2161188653759568,
      "grad_norm": 3.4769222736358643,
      "learning_rate": 0.00036036036036036037,
      "loss": 0.1648,
      "step": 240
    },
    {
      "epoch": 0.22512381809995496,
      "grad_norm": 0.6232728958129883,
      "learning_rate": 0.00037537537537537537,
      "loss": 0.1471,
      "step": 250
    },
    {
      "epoch": 0.23412877082395317,
      "grad_norm": 0.5445786714553833,
      "learning_rate": 0.0003903903903903904,
      "loss": 0.11,
      "step": 260
    },
    {
      "epoch": 0.24313372354795137,
      "grad_norm": 0.6299628615379333,
      "learning_rate": 0.00040540540540540544,
      "loss": 0.1469,
      "step": 270
    },
    {
      "epoch": 0.2521386762719496,
      "grad_norm": 0.5292666554450989,
      "learning_rate": 0.00042042042042042044,
      "loss": 0.128,
      "step": 280
    },
    {
      "epoch": 0.2611436289959478,
      "grad_norm": 0.4067528247833252,
      "learning_rate": 0.00043543543543543545,
      "loss": 0.1304,
      "step": 290
    },
    {
      "epoch": 0.270148581719946,
      "grad_norm": 0.3103502094745636,
      "learning_rate": 0.00045045045045045046,
      "loss": 0.1553,
      "step": 300
    },
    {
      "epoch": 0.2791535344439442,
      "grad_norm": 0.8202663064002991,
      "learning_rate": 0.00046546546546546546,
      "loss": 0.1307,
      "step": 310
    },
    {
      "epoch": 0.28815848716794235,
      "grad_norm": 0.8533945679664612,
      "learning_rate": 0.00048048048048048047,
      "loss": 0.1227,
      "step": 320
    },
    {
      "epoch": 0.29716343989194055,
      "grad_norm": 0.2991810739040375,
      "learning_rate": 0.0004954954954954955,
      "loss": 0.146,
      "step": 330
    },
    {
      "epoch": 0.30616839261593876,
      "grad_norm": 0.6459330916404724,
      "learning_rate": 0.0004999932697622858,
      "loss": 0.1388,
      "step": 340
    },
    {
      "epoch": 0.31517334533993696,
      "grad_norm": 1.311311960220337,
      "learning_rate": 0.0004999603062050034,
      "loss": 0.1578,
      "step": 350
    },
    {
      "epoch": 0.32417829806393517,
      "grad_norm": 0.738445520401001,
      "learning_rate": 0.0004998998767795804,
      "loss": 0.1262,
      "step": 360
    },
    {
      "epoch": 0.33318325078793337,
      "grad_norm": 0.5140581727027893,
      "learning_rate": 0.0004998119881260575,
      "loss": 0.1098,
      "step": 370
    },
    {
      "epoch": 0.3421882035119316,
      "grad_norm": 0.37815770506858826,
      "learning_rate": 0.0004996966499017208,
      "loss": 0.0832,
      "step": 380
    },
    {
      "epoch": 0.3511931562359298,
      "grad_norm": 0.31693974137306213,
      "learning_rate": 0.0004995538747800402,
      "loss": 0.1218,
      "step": 390
    },
    {
      "epoch": 0.360198108959928,
      "grad_norm": 0.39572128653526306,
      "learning_rate": 0.0004993836784492775,
      "loss": 0.111,
      "step": 400
    },
    {
      "epoch": 0.36920306168392614,
      "grad_norm": 1.249613881111145,
      "learning_rate": 0.0004991860796107617,
      "loss": 0.1086,
      "step": 410
    },
    {
      "epoch": 0.37820801440792434,
      "grad_norm": 0.481343150138855,
      "learning_rate": 0.0004989610999768349,
      "loss": 0.1213,
      "step": 420
    },
    {
      "epoch": 0.38721296713192255,
      "grad_norm": 0.4233911335468292,
      "learning_rate": 0.0004987087642684661,
      "loss": 0.1128,
      "step": 430
    },
    {
      "epoch": 0.39621791985592075,
      "grad_norm": 0.4806802570819855,
      "learning_rate": 0.0004984291002125344,
      "loss": 0.1166,
      "step": 440
    },
    {
      "epoch": 0.40522287257991896,
      "grad_norm": 0.48500722646713257,
      "learning_rate": 0.0004981221385387838,
      "loss": 0.1195,
      "step": 450
    },
    {
      "epoch": 0.41422782530391716,
      "grad_norm": 0.9949634671211243,
      "learning_rate": 0.0004977879129764447,
      "loss": 0.1077,
      "step": 460
    },
    {
      "epoch": 0.42323277802791537,
      "grad_norm": 0.6812320351600647,
      "learning_rate": 0.0004974264602505291,
      "loss": 0.1056,
      "step": 470
    },
    {
      "epoch": 0.4322377307519136,
      "grad_norm": 0.4456217288970947,
      "learning_rate": 0.0004970378200777949,
      "loss": 0.1288,
      "step": 480
    },
    {
      "epoch": 0.4412426834759118,
      "grad_norm": 0.282451868057251,
      "learning_rate": 0.0004966220351623811,
      "loss": 0.11,
      "step": 490
    },
    {
      "epoch": 0.45024763619990993,
      "grad_norm": 0.20943424105644226,
      "learning_rate": 0.0004961791511911165,
      "loss": 0.0964,
      "step": 500
    },
    {
      "epoch": 0.45925258892390813,
      "grad_norm": 0.693567156791687,
      "learning_rate": 0.0004957092168284987,
      "loss": 0.0884,
      "step": 510
    },
    {
      "epoch": 0.46825754164790634,
      "grad_norm": 0.5924340486526489,
      "learning_rate": 0.0004952122837113474,
      "loss": 0.0876,
      "step": 520
    },
    {
      "epoch": 0.47726249437190454,
      "grad_norm": 0.6064023971557617,
      "learning_rate": 0.00049468840644313,
      "loss": 0.0902,
      "step": 530
    },
    {
      "epoch": 0.48626744709590275,
      "grad_norm": 0.46677231788635254,
      "learning_rate": 0.0004941376425879623,
      "loss": 0.0979,
      "step": 540
    },
    {
      "epoch": 0.49527239981990095,
      "grad_norm": 0.7859164476394653,
      "learning_rate": 0.0004935600526642829,
      "loss": 0.0856,
      "step": 550
    },
    {
      "epoch": 0.5042773525438992,
      "grad_norm": 0.6721882224082947,
      "learning_rate": 0.0004929557001382031,
      "loss": 0.0986,
      "step": 560
    },
    {
      "epoch": 0.5132823052678973,
      "grad_norm": 2.158141851425171,
      "learning_rate": 0.0004923246514165339,
      "loss": 0.1308,
      "step": 570
    },
    {
      "epoch": 0.5222872579918956,
      "grad_norm": 0.24345816671848297,
      "learning_rate": 0.0004916669758394888,
      "loss": 0.0829,
      "step": 580
    },
    {
      "epoch": 0.5312922107158937,
      "grad_norm": 0.8264291882514954,
      "learning_rate": 0.0004909827456730646,
      "loss": 0.0892,
      "step": 590
    },
    {
      "epoch": 0.540297163439892,
      "grad_norm": 0.5661163330078125,
      "learning_rate": 0.0004902720361011007,
      "loss": 0.088,
      "step": 600
    },
    {
      "epoch": 0.5493021161638901,
      "grad_norm": 0.6446295380592346,
      "learning_rate": 0.0004895349252170179,
      "loss": 0.0825,
      "step": 610
    },
    {
      "epoch": 0.5583070688878884,
      "grad_norm": 0.8465402126312256,
      "learning_rate": 0.0004887714940152376,
      "loss": 0.0969,
      "step": 620
    },
    {
      "epoch": 0.5673120216118865,
      "grad_norm": 0.4694724380970001,
      "learning_rate": 0.00048798182638228163,
      "loss": 0.0833,
      "step": 630
    },
    {
      "epoch": 0.5763169743358847,
      "grad_norm": 0.6018555164337158,
      "learning_rate": 0.0004871660090875553,
      "loss": 0.088,
      "step": 640
    },
    {
      "epoch": 0.585321927059883,
      "grad_norm": 0.7449955940246582,
      "learning_rate": 0.0004863241317738125,
      "loss": 0.0912,
      "step": 650
    },
    {
      "epoch": 0.5943268797838811,
      "grad_norm": 0.3737930357456207,
      "learning_rate": 0.00048545628694730624,
      "loss": 0.098,
      "step": 660
    },
    {
      "epoch": 0.6033318325078794,
      "grad_norm": 0.5405699610710144,
      "learning_rate": 0.0004845625699676234,
      "loss": 0.1197,
      "step": 670
    },
    {
      "epoch": 0.6123367852318775,
      "grad_norm": 0.8846384882926941,
      "learning_rate": 0.0004836430790372071,
      "loss": 0.1038,
      "step": 680
    },
    {
      "epoch": 0.6213417379558758,
      "grad_norm": 0.7482890486717224,
      "learning_rate": 0.0004826979151905655,
      "loss": 0.0892,
      "step": 690
    },
    {
      "epoch": 0.6303466906798739,
      "grad_norm": 0.45233815908432007,
      "learning_rate": 0.0004817271822831708,
      "loss": 0.0866,
      "step": 700
    },
    {
      "epoch": 0.6393516434038722,
      "grad_norm": 0.5681776404380798,
      "learning_rate": 0.0004807309869800469,
      "loss": 0.1108,
      "step": 710
    },
    {
      "epoch": 0.6483565961278703,
      "grad_norm": 0.8282708525657654,
      "learning_rate": 0.00047970943874404905,
      "loss": 0.0892,
      "step": 720
    },
    {
      "epoch": 0.6573615488518685,
      "grad_norm": 0.9945095777511597,
      "learning_rate": 0.0004786626498238361,
      "loss": 0.0879,
      "step": 730
    },
    {
      "epoch": 0.6663665015758667,
      "grad_norm": 0.4571370780467987,
      "learning_rate": 0.00047759073524153667,
      "loss": 0.0937,
      "step": 740
    },
    {
      "epoch": 0.6753714542998649,
      "grad_norm": 0.790557861328125,
      "learning_rate": 0.00047649381278010996,
      "loss": 0.0743,
      "step": 750
    },
    {
      "epoch": 0.6843764070238632,
      "grad_norm": 1.6088427305221558,
      "learning_rate": 0.00047537200297040404,
      "loss": 0.3554,
      "step": 760
    },
    {
      "epoch": 0.6933813597478613,
      "grad_norm": 0.3656422197818756,
      "learning_rate": 0.00047422542907791176,
      "loss": 0.1057,
      "step": 770
    },
    {
      "epoch": 0.7023863124718596,
      "grad_norm": 0.4086725413799286,
      "learning_rate": 0.000473054217089226,
      "loss": 0.1145,
      "step": 780
    },
    {
      "epoch": 0.7113912651958577,
      "grad_norm": 0.9128967523574829,
      "learning_rate": 0.00047185849569819616,
      "loss": 0.0921,
      "step": 790
    },
    {
      "epoch": 0.720396217919856,
      "grad_norm": 0.22649647295475006,
      "learning_rate": 0.0004706383962917877,
      "loss": 0.0773,
      "step": 800
    },
    {
      "epoch": 0.7294011706438541,
      "grad_norm": 0.6621063947677612,
      "learning_rate": 0.0004693940529356444,
      "loss": 0.086,
      "step": 810
    },
    {
      "epoch": 0.7384061233678523,
      "grad_norm": 0.7384969592094421,
      "learning_rate": 0.0004681256023593579,
      "loss": 0.073,
      "step": 820
    },
    {
      "epoch": 0.7474110760918505,
      "grad_norm": 0.7373567223548889,
      "learning_rate": 0.00046683318394144285,
      "loss": 0.1029,
      "step": 830
    },
    {
      "epoch": 0.7564160288158487,
      "grad_norm": 0.5818125009536743,
      "learning_rate": 0.00046551693969402286,
      "loss": 0.0782,
      "step": 840
    },
    {
      "epoch": 0.765420981539847,
      "grad_norm": 0.25143295526504517,
      "learning_rate": 0.0004641770142472254,
      "loss": 0.0641,
      "step": 850
    },
    {
      "epoch": 0.7744259342638451,
      "grad_norm": 0.7186563611030579,
      "learning_rate": 0.00046281355483328957,
      "loss": 0.0605,
      "step": 860
    },
    {
      "epoch": 0.7834308869878434,
      "grad_norm": 1.8425294160842896,
      "learning_rate": 0.000461426711270389,
      "loss": 0.0876,
      "step": 870
    },
    {
      "epoch": 0.7924358397118415,
      "grad_norm": 0.6538152098655701,
      "learning_rate": 0.0004600166359461687,
      "loss": 0.0998,
      "step": 880
    },
    {
      "epoch": 0.8014407924358398,
      "grad_norm": 0.545005738735199,
      "learning_rate": 0.00045858348380100103,
      "loss": 0.0819,
      "step": 890
    },
    {
      "epoch": 0.8104457451598379,
      "grad_norm": 0.5454880595207214,
      "learning_rate": 0.00045712741231096054,
      "loss": 0.0777,
      "step": 900
    },
    {
      "epoch": 0.8194506978838361,
      "grad_norm": 0.32059967517852783,
      "learning_rate": 0.0004556485814705208,
      "loss": 0.059,
      "step": 910
    },
    {
      "epoch": 0.8284556506078343,
      "grad_norm": 0.7928904294967651,
      "learning_rate": 0.0004541471537749733,
      "loss": 0.0591,
      "step": 920
    },
    {
      "epoch": 0.8374606033318325,
      "grad_norm": 0.3916737735271454,
      "learning_rate": 0.000452623294202573,
      "loss": 0.0643,
      "step": 930
    },
    {
      "epoch": 0.8464655560558307,
      "grad_norm": 0.40121990442276,
      "learning_rate": 0.0004510771701964101,
      "loss": 0.0663,
      "step": 940
    },
    {
      "epoch": 0.8554705087798289,
      "grad_norm": 0.5519987344741821,
      "learning_rate": 0.00044950895164601106,
      "loss": 0.1189,
      "step": 950
    },
    {
      "epoch": 0.8644754615038271,
      "grad_norm": 0.3403993546962738,
      "learning_rate": 0.00044791881086867137,
      "loss": 0.0826,
      "step": 960
    },
    {
      "epoch": 0.8734804142278253,
      "grad_norm": 0.5146071910858154,
      "learning_rate": 0.00044630692259052074,
      "loss": 0.0734,
      "step": 970
    },
    {
      "epoch": 0.8824853669518236,
      "grad_norm": 0.984295129776001,
      "learning_rate": 0.00044467346392732454,
      "loss": 0.0476,
      "step": 980
    },
    {
      "epoch": 0.8914903196758217,
      "grad_norm": 0.4145427942276001,
      "learning_rate": 0.0004430186143650216,
      "loss": 0.076,
      "step": 990
    },
    {
      "epoch": 0.9004952723998199,
      "grad_norm": 0.6582645177841187,
      "learning_rate": 0.00044134255574000247,
      "loss": 0.0683,
      "step": 1000
    },
    {
      "epoch": 0.9095002251238181,
      "grad_norm": 0.3272668719291687,
      "learning_rate": 0.00043964547221912915,
      "loss": 0.0762,
      "step": 1010
    },
    {
      "epoch": 0.9185051778478163,
      "grad_norm": 2.123229503631592,
      "learning_rate": 0.0004379275502794983,
      "loss": 0.0784,
      "step": 1020
    },
    {
      "epoch": 0.9275101305718145,
      "grad_norm": 0.4320879876613617,
      "learning_rate": 0.0004361889786879514,
      "loss": 0.0556,
      "step": 1030
    },
    {
      "epoch": 0.9365150832958127,
      "grad_norm": 0.42792198061943054,
      "learning_rate": 0.0004344299484803325,
      "loss": 0.0715,
      "step": 1040
    },
    {
      "epoch": 0.9455200360198109,
      "grad_norm": 0.805536150932312,
      "learning_rate": 0.0004326506529404972,
      "loss": 0.0855,
      "step": 1050
    },
    {
      "epoch": 0.9545249887438091,
      "grad_norm": 0.3507474958896637,
      "learning_rate": 0.00043085128757907443,
      "loss": 0.1396,
      "step": 1060
    },
    {
      "epoch": 0.9635299414678073,
      "grad_norm": 1.1412338018417358,
      "learning_rate": 0.00042903205011198377,
      "loss": 0.0838,
      "step": 1070
    },
    {
      "epoch": 0.9725348941918055,
      "grad_norm": 0.6972079277038574,
      "learning_rate": 0.0004271931404387096,
      "loss": 0.0617,
      "step": 1080
    },
    {
      "epoch": 0.9815398469158036,
      "grad_norm": 0.5712110996246338,
      "learning_rate": 0.0004253347606203367,
      "loss": 0.0722,
      "step": 1090
    },
    {
      "epoch": 0.9905447996398019,
      "grad_norm": 1.1576753854751587,
      "learning_rate": 0.0004234571148573474,
      "loss": 0.0632,
      "step": 1100
    },
    {
      "epoch": 0.9995497523638001,
      "grad_norm": 0.8658812046051025,
      "learning_rate": 0.00042156040946718344,
      "loss": 0.1054,
      "step": 1110
    },
    {
      "epoch": 0.9995497523638001,
      "eval_loss": 0.10723324120044708,
      "eval_runtime": 366.6207,
      "eval_samples_per_second": 3.404,
      "eval_steps_per_second": 0.851,
      "step": 1110
    },
    {
      "epoch": 1.0085547050877983,
      "grad_norm": 0.23787397146224976,
      "learning_rate": 0.00041964485286157595,
      "loss": 0.0472,
      "step": 1120
    },
    {
      "epoch": 1.0175596578117965,
      "grad_norm": 0.7538590431213379,
      "learning_rate": 0.0004177106555236451,
      "loss": 0.057,
      "step": 1130
    },
    {
      "epoch": 1.0265646105357946,
      "grad_norm": 1.0165269374847412,
      "learning_rate": 0.0004157580299847717,
      "loss": 0.0392,
      "step": 1140
    },
    {
      "epoch": 1.035569563259793,
      "grad_norm": 0.7899549007415771,
      "learning_rate": 0.00041378719080124426,
      "loss": 0.0545,
      "step": 1150
    },
    {
      "epoch": 1.0445745159837911,
      "grad_norm": 0.7690303921699524,
      "learning_rate": 0.00041179835453068346,
      "loss": 0.1168,
      "step": 1160
    },
    {
      "epoch": 1.0535794687077893,
      "grad_norm": 0.19441792368888855,
      "learning_rate": 0.0004097917397082462,
      "loss": 0.0505,
      "step": 1170
    },
    {
      "epoch": 1.0625844214317874,
      "grad_norm": 0.7040566205978394,
      "learning_rate": 0.00040776756682261317,
      "loss": 0.0439,
      "step": 1180
    },
    {
      "epoch": 1.0715893741557856,
      "grad_norm": 0.531909167766571,
      "learning_rate": 0.00040572605829176104,
      "loss": 0.0499,
      "step": 1190
    },
    {
      "epoch": 1.080594326879784,
      "grad_norm": 1.213708519935608,
      "learning_rate": 0.0004036674384385231,
      "loss": 0.0515,
      "step": 1200
    },
    {
      "epoch": 1.0895992796037821,
      "grad_norm": 0.7988349795341492,
      "learning_rate": 0.0004015919334659407,
      "loss": 0.0521,
      "step": 1210
    },
    {
      "epoch": 1.0986042323277803,
      "grad_norm": 0.6211755275726318,
      "learning_rate": 0.00039949977143240737,
      "loss": 0.057,
      "step": 1220
    },
    {
      "epoch": 1.1076091850517784,
      "grad_norm": 0.3487605154514313,
      "learning_rate": 0.00039739118222660986,
      "loss": 0.0598,
      "step": 1230
    },
    {
      "epoch": 1.1166141377757768,
      "grad_norm": 0.9181510210037231,
      "learning_rate": 0.00039526639754226784,
      "loss": 0.0439,
      "step": 1240
    },
    {
      "epoch": 1.125619090499775,
      "grad_norm": 0.8258150219917297,
      "learning_rate": 0.00039312565085267495,
      "loss": 0.0635,
      "step": 1250
    },
    {
      "epoch": 1.134624043223773,
      "grad_norm": 0.5675126910209656,
      "learning_rate": 0.00039096917738504444,
      "loss": 0.0549,
      "step": 1260
    },
    {
      "epoch": 1.1436289959477712,
      "grad_norm": 0.9179567098617554,
      "learning_rate": 0.00038879721409466287,
      "loss": 0.0525,
      "step": 1270
    },
    {
      "epoch": 1.1526339486717694,
      "grad_norm": 0.660239040851593,
      "learning_rate": 0.0003866099996388522,
      "loss": 0.0406,
      "step": 1280
    },
    {
      "epoch": 1.1616389013957678,
      "grad_norm": 0.28137487173080444,
      "learning_rate": 0.0003844077743507468,
      "loss": 0.0507,
      "step": 1290
    },
    {
      "epoch": 1.170643854119766,
      "grad_norm": 1.0400328636169434,
      "learning_rate": 0.0003821907802128851,
      "loss": 0.0415,
      "step": 1300
    },
    {
      "epoch": 1.179648806843764,
      "grad_norm": 0.31403201818466187,
      "learning_rate": 0.00037995926083062015,
      "loss": 0.0413,
      "step": 1310
    },
    {
      "epoch": 1.1886537595677622,
      "grad_norm": 0.7753293514251709,
      "learning_rate": 0.0003777134614053522,
      "loss": 0.0539,
      "step": 1320
    },
    {
      "epoch": 1.1976587122917604,
      "grad_norm": 0.8726606369018555,
      "learning_rate": 0.0003754536287075859,
      "loss": 0.0447,
      "step": 1330
    },
    {
      "epoch": 1.2066636650157587,
      "grad_norm": 0.6008639335632324,
      "learning_rate": 0.0003731800110498147,
      "loss": 0.0525,
      "step": 1340
    },
    {
      "epoch": 1.2156686177397569,
      "grad_norm": 0.6586739420890808,
      "learning_rate": 0.00037089285825923614,
      "loss": 0.0471,
      "step": 1350
    },
    {
      "epoch": 1.224673570463755,
      "grad_norm": 0.772767186164856,
      "learning_rate": 0.0003685924216503005,
      "loss": 0.0514,
      "step": 1360
    },
    {
      "epoch": 1.2336785231877532,
      "grad_norm": 0.8908634781837463,
      "learning_rate": 0.0003662789539970964,
      "loss": 0.079,
      "step": 1370
    },
    {
      "epoch": 1.2426834759117515,
      "grad_norm": 0.517117977142334,
      "learning_rate": 0.0003639527095055753,
      "loss": 0.0468,
      "step": 1380
    },
    {
      "epoch": 1.2516884286357497,
      "grad_norm": 0.10728801786899567,
      "learning_rate": 0.0003616139437856198,
      "loss": 0.0525,
      "step": 1390
    },
    {
      "epoch": 1.2606933813597478,
      "grad_norm": 1.347013235092163,
      "learning_rate": 0.0003592629138229562,
      "loss": 0.0457,
      "step": 1400
    },
    {
      "epoch": 1.269698334083746,
      "grad_norm": 0.7579188942909241,
      "learning_rate": 0.00035689987795091736,
      "loss": 0.0507,
      "step": 1410
    },
    {
      "epoch": 1.2787032868077444,
      "grad_norm": 1.1709978580474854,
      "learning_rate": 0.00035452509582205614,
      "loss": 0.0473,
      "step": 1420
    },
    {
      "epoch": 1.2877082395317425,
      "grad_norm": 0.5656291246414185,
      "learning_rate": 0.0003521388283796152,
      "loss": 0.0476,
      "step": 1430
    },
    {
      "epoch": 1.2967131922557407,
      "grad_norm": 1.290003776550293,
      "learning_rate": 0.00034974133782885407,
      "loss": 0.0455,
      "step": 1440
    },
    {
      "epoch": 1.3057181449797388,
      "grad_norm": 0.7571908235549927,
      "learning_rate": 0.00034733288760823745,
      "loss": 0.0606,
      "step": 1450
    },
    {
      "epoch": 1.314723097703737,
      "grad_norm": 0.5460188388824463,
      "learning_rate": 0.00034491374236048875,
      "loss": 0.0522,
      "step": 1460
    },
    {
      "epoch": 1.3237280504277353,
      "grad_norm": 0.6553800702095032,
      "learning_rate": 0.00034248416790351086,
      "loss": 0.0471,
      "step": 1470
    },
    {
      "epoch": 1.3327330031517335,
      "grad_norm": 0.12242508679628372,
      "learning_rate": 0.00034004443120117764,
      "loss": 0.031,
      "step": 1480
    },
    {
      "epoch": 1.3417379558757316,
      "grad_norm": 0.30565592646598816,
      "learning_rate": 0.0003375948003339999,
      "loss": 0.0582,
      "step": 1490
    },
    {
      "epoch": 1.3507429085997298,
      "grad_norm": 1.204742431640625,
      "learning_rate": 0.00033513554446966843,
      "loss": 0.0637,
      "step": 1500
    },
    {
      "epoch": 1.359747861323728,
      "grad_norm": 0.3687357008457184,
      "learning_rate": 0.0003326669338334774,
      "loss": 0.0395,
      "step": 1510
    },
    {
      "epoch": 1.3687528140477263,
      "grad_norm": 0.30386462807655334,
      "learning_rate": 0.000330189239678632,
      "loss": 0.0383,
      "step": 1520
    },
    {
      "epoch": 1.3777577667717245,
      "grad_norm": 0.8197789788246155,
      "learning_rate": 0.00032770273425644284,
      "loss": 0.0401,
      "step": 1530
    },
    {
      "epoch": 1.3867627194957226,
      "grad_norm": 1.0078364610671997,
      "learning_rate": 0.0003252076907864105,
      "loss": 0.036,
      "step": 1540
    },
    {
      "epoch": 1.395767672219721,
      "grad_norm": 0.6033559441566467,
      "learning_rate": 0.00032270438342620435,
      "loss": 0.0321,
      "step": 1550
    },
    {
      "epoch": 1.4047726249437191,
      "grad_norm": 0.4948568344116211,
      "learning_rate": 0.0003201930872415374,
      "loss": 0.0327,
      "step": 1560
    },
    {
      "epoch": 1.4137775776677173,
      "grad_norm": 0.5108727812767029,
      "learning_rate": 0.00031767407817594206,
      "loss": 0.0431,
      "step": 1570
    },
    {
      "epoch": 1.4227825303917154,
      "grad_norm": 0.4678942561149597,
      "learning_rate": 0.0003151476330204494,
      "loss": 0.0572,
      "step": 1580
    },
    {
      "epoch": 1.4317874831157136,
      "grad_norm": 0.4139574468135834,
      "learning_rate": 0.0003126140293831746,
      "loss": 0.0511,
      "step": 1590
    },
    {
      "epoch": 1.440792435839712,
      "grad_norm": 0.3055359125137329,
      "learning_rate": 0.0003100735456588136,
      "loss": 0.0336,
      "step": 1600
    },
    {
      "epoch": 1.44979738856371,
      "grad_norm": 1.0992532968521118,
      "learning_rate": 0.0003075264609980525,
      "loss": 0.0462,
      "step": 1610
    },
    {
      "epoch": 1.4588023412877082,
      "grad_norm": 0.45761075615882874,
      "learning_rate": 0.00030497305527689444,
      "loss": 0.0416,
      "step": 1620
    },
    {
      "epoch": 1.4678072940117064,
      "grad_norm": 0.40889444947242737,
      "learning_rate": 0.00030241360906590634,
      "loss": 0.0416,
      "step": 1630
    },
    {
      "epoch": 1.4768122467357045,
      "grad_norm": 0.2063206434249878,
      "learning_rate": 0.00029984840359939,
      "loss": 0.0325,
      "step": 1640
    },
    {
      "epoch": 1.485817199459703,
      "grad_norm": 0.7593374848365784,
      "learning_rate": 0.0002972777207444791,
      "loss": 0.0458,
      "step": 1650
    },
    {
      "epoch": 1.494822152183701,
      "grad_norm": 0.4696616232395172,
      "learning_rate": 0.0002947018429701681,
      "loss": 0.0382,
      "step": 1660
    },
    {
      "epoch": 1.5038271049076992,
      "grad_norm": 0.7529333829879761,
      "learning_rate": 0.0002921210533162739,
      "loss": 0.0539,
      "step": 1670
    },
    {
      "epoch": 1.5128320576316976,
      "grad_norm": 0.3244212865829468,
      "learning_rate": 0.0002895356353623352,
      "loss": 0.0399,
      "step": 1680
    },
    {
      "epoch": 1.5218370103556955,
      "grad_norm": 0.6148865222930908,
      "learning_rate": 0.0002869458731964526,
      "loss": 0.035,
      "step": 1690
    },
    {
      "epoch": 1.530841963079694,
      "grad_norm": 0.5794234871864319,
      "learning_rate": 0.0002843520513840727,
      "loss": 0.0367,
      "step": 1700
    },
    {
      "epoch": 1.539846915803692,
      "grad_norm": 2.3223016262054443,
      "learning_rate": 0.0002817544549367197,
      "loss": 0.0228,
      "step": 1710
    },
    {
      "epoch": 1.5488518685276902,
      "grad_norm": 0.8081986308097839,
      "learning_rate": 0.0002791533692806782,
      "loss": 0.0367,
      "step": 1720
    },
    {
      "epoch": 1.5578568212516886,
      "grad_norm": 0.8312768936157227,
      "learning_rate": 0.00027654908022563057,
      "loss": 0.0331,
      "step": 1730
    },
    {
      "epoch": 1.5668617739756865,
      "grad_norm": 1.3901196718215942,
      "learning_rate": 0.00027394187393325106,
      "loss": 0.0516,
      "step": 1740
    },
    {
      "epoch": 1.5758667266996849,
      "grad_norm": 0.26659074425697327,
      "learning_rate": 0.0002713320368857629,
      "loss": 0.034,
      "step": 1750
    },
    {
      "epoch": 1.584871679423683,
      "grad_norm": 0.35972025990486145,
      "learning_rate": 0.00026871985585445924,
      "loss": 0.0394,
      "step": 1760
    },
    {
      "epoch": 1.5938766321476812,
      "grad_norm": 0.4983198344707489,
      "learning_rate": 0.00026610561786819204,
      "loss": 0.0541,
      "step": 1770
    },
    {
      "epoch": 1.6028815848716795,
      "grad_norm": 0.28968626260757446,
      "learning_rate": 0.0002634896101818337,
      "loss": 0.0362,
      "step": 1780
    },
    {
      "epoch": 1.6118865375956775,
      "grad_norm": 0.5133367776870728,
      "learning_rate": 0.00026087212024471267,
      "loss": 0.0379,
      "step": 1790
    },
    {
      "epoch": 1.6208914903196758,
      "grad_norm": 0.5385307669639587,
      "learning_rate": 0.00025825343566902837,
      "loss": 0.0235,
      "step": 1800
    },
    {
      "epoch": 1.629896443043674,
      "grad_norm": 0.2894695997238159,
      "learning_rate": 0.0002556338441982486,
      "loss": 0.0471,
      "step": 1810
    },
    {
      "epoch": 1.6389013957676721,
      "grad_norm": 0.1257040649652481,
      "learning_rate": 0.00025301363367549116,
      "loss": 0.0351,
      "step": 1820
    },
    {
      "epoch": 1.6479063484916705,
      "grad_norm": 0.48474758863449097,
      "learning_rate": 0.00025039309201189616,
      "loss": 0.0299,
      "step": 1830
    },
    {
      "epoch": 1.6569113012156687,
      "grad_norm": 0.31330806016921997,
      "learning_rate": 0.00024777250715498966,
      "loss": 0.0345,
      "step": 1840
    },
    {
      "epoch": 1.6659162539396668,
      "grad_norm": 0.26710548996925354,
      "learning_rate": 0.0002451521670570439,
      "loss": 0.0372,
      "step": 1850
    },
    {
      "epoch": 1.6749212066636652,
      "grad_norm": 0.6594274640083313,
      "learning_rate": 0.00024253235964343675,
      "loss": 0.0406,
      "step": 1860
    },
    {
      "epoch": 1.683926159387663,
      "grad_norm": 5.2465996742248535,
      "learning_rate": 0.00023991337278101414,
      "loss": 0.021,
      "step": 1870
    },
    {
      "epoch": 1.6929311121116615,
      "grad_norm": 0.5682045221328735,
      "learning_rate": 0.00023729549424645916,
      "loss": 0.0368,
      "step": 1880
    },
    {
      "epoch": 1.7019360648356596,
      "grad_norm": 0.08000780642032623,
      "learning_rate": 0.00023467901169467098,
      "loss": 0.0236,
      "step": 1890
    },
    {
      "epoch": 1.7109410175596578,
      "grad_norm": 0.39310336112976074,
      "learning_rate": 0.00023206421262715656,
      "loss": 0.0274,
      "step": 1900
    },
    {
      "epoch": 1.7199459702836561,
      "grad_norm": 0.38064146041870117,
      "learning_rate": 0.00022945138436044027,
      "loss": 0.0182,
      "step": 1910
    },
    {
      "epoch": 1.728950923007654,
      "grad_norm": 0.2647002339363098,
      "learning_rate": 0.00022684081399449325,
      "loss": 0.0341,
      "step": 1920
    },
    {
      "epoch": 1.7379558757316524,
      "grad_norm": 0.7339056134223938,
      "learning_rate": 0.00022423278838118593,
      "loss": 0.0241,
      "step": 1930
    },
    {
      "epoch": 1.7469608284556506,
      "grad_norm": 0.1947578340768814,
      "learning_rate": 0.00022162759409276946,
      "loss": 0.0635,
      "step": 1940
    },
    {
      "epoch": 1.7559657811796487,
      "grad_norm": 0.44755756855010986,
      "learning_rate": 0.00021902551739038623,
      "loss": 0.0343,
      "step": 1950
    },
    {
      "epoch": 1.7649707339036471,
      "grad_norm": 1.5070298910140991,
      "learning_rate": 0.00021642684419261537,
      "loss": 0.0398,
      "step": 1960
    },
    {
      "epoch": 1.773975686627645,
      "grad_norm": 0.5109950304031372,
      "learning_rate": 0.0002138318600440559,
      "loss": 0.0269,
      "step": 1970
    },
    {
      "epoch": 1.7829806393516434,
      "grad_norm": 0.8602770566940308,
      "learning_rate": 0.00021124085008395051,
      "loss": 0.0248,
      "step": 1980
    },
    {
      "epoch": 1.7919855920756416,
      "grad_norm": 1.3361340761184692,
      "learning_rate": 0.0002086540990148548,
      "loss": 0.0367,
      "step": 1990
    },
    {
      "epoch": 1.8009905447996397,
      "grad_norm": 0.5540564656257629,
      "learning_rate": 0.00020607189107135302,
      "loss": 0.0356,
      "step": 2000
    },
    {
      "epoch": 1.809995497523638,
      "grad_norm": 0.400104820728302,
      "learning_rate": 0.000203494509988827,
      "loss": 0.0248,
      "step": 2010
    },
    {
      "epoch": 1.8190004502476362,
      "grad_norm": 0.24361105263233185,
      "learning_rate": 0.00020092223897227854,
      "loss": 0.0196,
      "step": 2020
    },
    {
      "epoch": 1.8280054029716344,
      "grad_norm": 0.431636244058609,
      "learning_rate": 0.0001983553606652105,
      "loss": 0.0171,
      "step": 2030
    },
    {
      "epoch": 1.8370103556956328,
      "grad_norm": 1.4231510162353516,
      "learning_rate": 0.0001957941571185702,
      "loss": 0.0411,
      "step": 2040
    },
    {
      "epoch": 1.8460153084196307,
      "grad_norm": 0.8232686519622803,
      "learning_rate": 0.00019323890975975688,
      "loss": 0.0295,
      "step": 2050
    },
    {
      "epoch": 1.855020261143629,
      "grad_norm": 0.27120670676231384,
      "learning_rate": 0.00019068989936169825,
      "loss": 0.0195,
      "step": 2060
    },
    {
      "epoch": 1.8640252138676272,
      "grad_norm": 0.5504851937294006,
      "learning_rate": 0.0001881474060119994,
      "loss": 0.0232,
      "step": 2070
    },
    {
      "epoch": 1.8730301665916254,
      "grad_norm": 0.8906576633453369,
      "learning_rate": 0.00018561170908216584,
      "loss": 0.0383,
      "step": 2080
    },
    {
      "epoch": 1.8820351193156237,
      "grad_norm": 0.37249383330345154,
      "learning_rate": 0.00018308308719690642,
      "loss": 0.0356,
      "step": 2090
    },
    {
      "epoch": 1.8910400720396217,
      "grad_norm": 0.5535292029380798,
      "learning_rate": 0.00018056181820351737,
      "loss": 0.0249,
      "step": 2100
    },
    {
      "epoch": 1.90004502476362,
      "grad_norm": 0.40757960081100464,
      "learning_rate": 0.0001780481791413527,
      "loss": 0.0123,
      "step": 2110
    },
    {
      "epoch": 1.9090499774876182,
      "grad_norm": 0.19524797797203064,
      "learning_rate": 0.00017554244621138226,
      "loss": 0.0333,
      "step": 2120
    },
    {
      "epoch": 1.9180549302116163,
      "grad_norm": 0.24032080173492432,
      "learning_rate": 0.00017304489474584305,
      "loss": 0.0187,
      "step": 2130
    },
    {
      "epoch": 1.9270598829356147,
      "grad_norm": 0.6353605389595032,
      "learning_rate": 0.00017055579917798526,
      "loss": 0.0317,
      "step": 2140
    },
    {
      "epoch": 1.9360648356596126,
      "grad_norm": 0.03512892499566078,
      "learning_rate": 0.00016807543301191696,
      "loss": 0.0224,
      "step": 2150
    },
    {
      "epoch": 1.945069788383611,
      "grad_norm": 0.1845867782831192,
      "learning_rate": 0.00016560406879255192,
      "loss": 0.0326,
      "step": 2160
    },
    {
      "epoch": 1.9540747411076091,
      "grad_norm": 0.19545306265354156,
      "learning_rate": 0.0001631419780756616,
      "loss": 0.0302,
      "step": 2170
    },
    {
      "epoch": 1.9630796938316073,
      "grad_norm": 0.258370578289032,
      "learning_rate": 0.00016068943139803624,
      "loss": 0.0215,
      "step": 2180
    },
    {
      "epoch": 1.9720846465556057,
      "grad_norm": 1.101582646369934,
      "learning_rate": 0.00015824669824775867,
      "loss": 0.0235,
      "step": 2190
    },
    {
      "epoch": 1.9810895992796038,
      "grad_norm": 0.48038938641548157,
      "learning_rate": 0.00015581404703459215,
      "loss": 0.0161,
      "step": 2200
    },
    {
      "epoch": 1.990094552003602,
      "grad_norm": 0.2140682339668274,
      "learning_rate": 0.0001533917450604872,
      "loss": 0.0317,
      "step": 2210
    },
    {
      "epoch": 1.9990995047276003,
      "grad_norm": 0.353293776512146,
      "learning_rate": 0.0001509800584902108,
      "loss": 0.0321,
      "step": 2220
    },
    {
      "epoch": 2.0,
      "eval_loss": 0.02353656478226185,
      "eval_runtime": 367.1994,
      "eval_samples_per_second": 3.399,
      "eval_steps_per_second": 0.85,
      "step": 2221
    },
    {
      "epoch": 2.0081044574515983,
      "grad_norm": 0.12764601409435272,
      "learning_rate": 0.00014857925232209952,
      "loss": 0.0138,
      "step": 2230
    },
    {
      "epoch": 2.0171094101755966,
      "grad_norm": 0.024980181828141212,
      "learning_rate": 0.0001461895903589411,
      "loss": 0.0052,
      "step": 2240
    },
    {
      "epoch": 2.0261143628995946,
      "grad_norm": 0.039625007659196854,
      "learning_rate": 0.00014381133517898804,
      "loss": 0.0056,
      "step": 2250
    },
    {
      "epoch": 2.035119315623593,
      "grad_norm": 0.06637617200613022,
      "learning_rate": 0.00014144474810710506,
      "loss": 0.0052,
      "step": 2260
    },
    {
      "epoch": 2.0441242683475913,
      "grad_norm": 0.007026110775768757,
      "learning_rate": 0.0001390900891860542,
      "loss": 0.0038,
      "step": 2270
    },
    {
      "epoch": 2.0531292210715892,
      "grad_norm": 0.022180603817105293,
      "learning_rate": 0.00013674761714792152,
      "loss": 0.0029,
      "step": 2280
    },
    {
      "epoch": 2.0621341737955876,
      "grad_norm": 0.0910431295633316,
      "learning_rate": 0.00013441758938568702,
      "loss": 0.0112,
      "step": 2290
    },
    {
      "epoch": 2.071139126519586,
      "grad_norm": 0.024010568857192993,
      "learning_rate": 0.00013210026192494263,
      "loss": 0.0106,
      "step": 2300
    },
    {
      "epoch": 2.080144079243584,
      "grad_norm": 0.05744105577468872,
      "learning_rate": 0.0001297958893957588,
      "loss": 0.0028,
      "step": 2310
    },
    {
      "epoch": 2.0891490319675823,
      "grad_norm": 0.014688246883451939,
      "learning_rate": 0.00012750472500470684,
      "loss": 0.003,
      "step": 2320
    },
    {
      "epoch": 2.09815398469158,
      "grad_norm": 0.47415363788604736,
      "learning_rate": 0.00012522702050703539,
      "loss": 0.004,
      "step": 2330
    },
    {
      "epoch": 2.1071589374155786,
      "grad_norm": 0.010614756494760513,
      "learning_rate": 0.00012296302617900771,
      "loss": 0.0039,
      "step": 2340
    },
    {
      "epoch": 2.116163890139577,
      "grad_norm": 0.005130590405315161,
      "learning_rate": 0.00012071299079040093,
      "loss": 0.0052,
      "step": 2350
    },
    {
      "epoch": 2.125168842863575,
      "grad_norm": 0.00897208321839571,
      "learning_rate": 0.00011847716157717125,
      "loss": 0.0012,
      "step": 2360
    },
    {
      "epoch": 2.1341737955875733,
      "grad_norm": 0.018590709194540977,
      "learning_rate": 0.00011625578421428714,
      "loss": 0.0041,
      "step": 2370
    },
    {
      "epoch": 2.143178748311571,
      "grad_norm": 0.007833139970898628,
      "learning_rate": 0.00011404910278873442,
      "loss": 0.0042,
      "step": 2380
    },
    {
      "epoch": 2.1521837010355696,
      "grad_norm": 0.4387027621269226,
      "learning_rate": 0.00011185735977269618,
      "loss": 0.0045,
      "step": 2390
    },
    {
      "epoch": 2.161188653759568,
      "grad_norm": 0.03782733529806137,
      "learning_rate": 0.00010968079599690872,
      "loss": 0.0107,
      "step": 2400
    },
    {
      "epoch": 2.170193606483566,
      "grad_norm": 1.7100073099136353,
      "learning_rate": 0.00010751965062420008,
      "loss": 0.0132,
      "step": 2410
    },
    {
      "epoch": 2.1791985592075642,
      "grad_norm": 0.30084243416786194,
      "learning_rate": 0.00010537416112320966,
      "loss": 0.0032,
      "step": 2420
    },
    {
      "epoch": 2.1882035119315626,
      "grad_norm": 0.09218510240316391,
      "learning_rate": 0.00010324456324229536,
      "loss": 0.0133,
      "step": 2430
    },
    {
      "epoch": 2.1972084646555605,
      "grad_norm": 0.04099499061703682,
      "learning_rate": 0.00010113109098362933,
      "loss": 0.0042,
      "step": 2440
    },
    {
      "epoch": 2.206213417379559,
      "grad_norm": 0.11786336451768875,
      "learning_rate": 9.90339765774854e-05,
      "loss": 0.0079,
      "step": 2450
    },
    {
      "epoch": 2.215218370103557,
      "grad_norm": 0.21022629737854004,
      "learning_rate": 9.695345045672167e-05,
      "loss": 0.0068,
      "step": 2460
    },
    {
      "epoch": 2.224223322827555,
      "grad_norm": 0.003679335117340088,
      "learning_rate": 9.488974123145999e-05,
      "loss": 0.0065,
      "step": 2470
    },
    {
      "epoch": 2.2332282755515536,
      "grad_norm": 0.3586430549621582,
      "learning_rate": 9.284307566396677e-05,
      "loss": 0.007,
      "step": 2480
    },
    {
      "epoch": 2.2422332282755515,
      "grad_norm": 0.05965292453765869,
      "learning_rate": 9.081367864373488e-05,
      "loss": 0.0036,
      "step": 2490
    },
    {
      "epoch": 2.25123818099955,
      "grad_norm": 0.07229771465063095,
      "learning_rate": 8.880177316277404e-05,
      "loss": 0.003,
      "step": 2500
    },
    {
      "epoch": 2.260243133723548,
      "grad_norm": 0.025478554889559746,
      "learning_rate": 8.680758029110722e-05,
      "loss": 0.0031,
      "step": 2510
    },
    {
      "epoch": 2.269248086447546,
      "grad_norm": 0.21489132940769196,
      "learning_rate": 8.483131915247969e-05,
      "loss": 0.0079,
      "step": 2520
    },
    {
      "epoch": 2.2782530391715445,
      "grad_norm": 0.0027595176361501217,
      "learning_rate": 8.287320690028127e-05,
      "loss": 0.0038,
      "step": 2530
    },
    {
      "epoch": 2.2872579918955425,
      "grad_norm": 0.04456726834177971,
      "learning_rate": 8.093345869368588e-05,
      "loss": 0.0098,
      "step": 2540
    },
    {
      "epoch": 2.296262944619541,
      "grad_norm": 0.45820796489715576,
      "learning_rate": 7.901228767400859e-05,
      "loss": 0.0058,
      "step": 2550
    },
    {
      "epoch": 2.3052678973435388,
      "grad_norm": 0.023580515757203102,
      "learning_rate": 7.71099049412867e-05,
      "loss": 0.0056,
      "step": 2560
    },
    {
      "epoch": 2.314272850067537,
      "grad_norm": 0.009128079749643803,
      "learning_rate": 7.522651953108298e-05,
      "loss": 0.0028,
      "step": 2570
    },
    {
      "epoch": 2.3232778027915355,
      "grad_norm": 0.019036557525396347,
      "learning_rate": 7.336233839151694e-05,
      "loss": 0.0076,
      "step": 2580
    },
    {
      "epoch": 2.3322827555155334,
      "grad_norm": 0.4838591516017914,
      "learning_rate": 7.151756636052528e-05,
      "loss": 0.0134,
      "step": 2590
    },
    {
      "epoch": 2.341287708239532,
      "grad_norm": 0.0369805246591568,
      "learning_rate": 6.969240614335388e-05,
      "loss": 0.0029,
      "step": 2600
    },
    {
      "epoch": 2.3502926609635297,
      "grad_norm": 0.08291290700435638,
      "learning_rate": 6.788705829028483e-05,
      "loss": 0.0059,
      "step": 2610
    },
    {
      "epoch": 2.359297613687528,
      "grad_norm": 0.1803552657365799,
      "learning_rate": 6.610172117459889e-05,
      "loss": 0.0043,
      "step": 2620
    },
    {
      "epoch": 2.3683025664115265,
      "grad_norm": 0.2633506655693054,
      "learning_rate": 6.433659097077915e-05,
      "loss": 0.0058,
      "step": 2630
    },
    {
      "epoch": 2.3773075191355244,
      "grad_norm": 0.3316781520843506,
      "learning_rate": 6.259186163295438e-05,
      "loss": 0.0013,
      "step": 2640
    },
    {
      "epoch": 2.3863124718595228,
      "grad_norm": 0.05800594389438629,
      "learning_rate": 6.086772487358746e-05,
      "loss": 0.0044,
      "step": 2650
    },
    {
      "epoch": 2.3953174245835207,
      "grad_norm": 0.008593659847974777,
      "learning_rate": 5.9164370142409884e-05,
      "loss": 0.0138,
      "step": 2660
    },
    {
      "epoch": 2.404322377307519,
      "grad_norm": 0.1065651997923851,
      "learning_rate": 5.7481984605604745e-05,
      "loss": 0.0019,
      "step": 2670
    },
    {
      "epoch": 2.4133273300315174,
      "grad_norm": 0.06452436745166779,
      "learning_rate": 5.582075312524082e-05,
      "loss": 0.006,
      "step": 2680
    },
    {
      "epoch": 2.4223322827555154,
      "grad_norm": 0.022920764982700348,
      "learning_rate": 5.4180858238959684e-05,
      "loss": 0.0075,
      "step": 2690
    },
    {
      "epoch": 2.4313372354795137,
      "grad_norm": 0.041320864111185074,
      "learning_rate": 5.256248013991857e-05,
      "loss": 0.0007,
      "step": 2700
    },
    {
      "epoch": 2.440342188203512,
      "grad_norm": 0.08766137063503265,
      "learning_rate": 5.0965796656989886e-05,
      "loss": 0.0006,
      "step": 2710
    },
    {
      "epoch": 2.44934714092751,
      "grad_norm": 0.19868572056293488,
      "learning_rate": 4.9390983235222176e-05,
      "loss": 0.0015,
      "step": 2720
    },
    {
      "epoch": 2.4583520936515084,
      "grad_norm": 0.1469186395406723,
      "learning_rate": 4.783821291656129e-05,
      "loss": 0.001,
      "step": 2730
    },
    {
      "epoch": 2.4673570463755063,
      "grad_norm": 0.011856943368911743,
      "learning_rate": 4.6307656320836724e-05,
      "loss": 0.0041,
      "step": 2740
    },
    {
      "epoch": 2.4763619990995047,
      "grad_norm": 0.02158079482614994,
      "learning_rate": 4.479948162701364e-05,
      "loss": 0.0023,
      "step": 2750
    },
    {
      "epoch": 2.485366951823503,
      "grad_norm": 0.2254478931427002,
      "learning_rate": 4.3313854554713454e-05,
      "loss": 0.0076,
      "step": 2760
    },
    {
      "epoch": 2.494371904547501,
      "grad_norm": 0.007029518019407988,
      "learning_rate": 4.185093834600381e-05,
      "loss": 0.0034,
      "step": 2770
    },
    {
      "epoch": 2.5033768572714994,
      "grad_norm": 0.25719398260116577,
      "learning_rate": 4.0410893747462236e-05,
      "loss": 0.0048,
      "step": 2780
    },
    {
      "epoch": 2.5123818099954978,
      "grad_norm": 0.36692216992378235,
      "learning_rate": 3.8993878992512414e-05,
      "loss": 0.0072,
      "step": 2790
    },
    {
      "epoch": 2.5213867627194957,
      "grad_norm": 0.006349192000925541,
      "learning_rate": 3.7600049784037806e-05,
      "loss": 0.0023,
      "step": 2800
    },
    {
      "epoch": 2.530391715443494,
      "grad_norm": 1.0760622024536133,
      "learning_rate": 3.6229559277272614e-05,
      "loss": 0.0107,
      "step": 2810
    },
    {
      "epoch": 2.539396668167492,
      "grad_norm": 0.07895110547542572,
      "learning_rate": 3.488255806297311e-05,
      "loss": 0.0037,
      "step": 2820
    },
    {
      "epoch": 2.5484016208914904,
      "grad_norm": 0.01578269526362419,
      "learning_rate": 3.3559194150870526e-05,
      "loss": 0.0052,
      "step": 2830
    },
    {
      "epoch": 2.5574065736154887,
      "grad_norm": 0.694921612739563,
      "learning_rate": 3.2259612953407544e-05,
      "loss": 0.0055,
      "step": 2840
    },
    {
      "epoch": 2.5664115263394867,
      "grad_norm": 0.005983259528875351,
      "learning_rate": 3.09839572697605e-05,
      "loss": 0.0016,
      "step": 2850
    },
    {
      "epoch": 2.575416479063485,
      "grad_norm": 0.001576392212882638,
      "learning_rate": 2.973236727014797e-05,
      "loss": 0.0009,
      "step": 2860
    },
    {
      "epoch": 2.584421431787483,
      "grad_norm": 0.24479621648788452,
      "learning_rate": 2.850498048042935e-05,
      "loss": 0.0021,
      "step": 2870
    },
    {
      "epoch": 2.5934263845114813,
      "grad_norm": 0.04124682396650314,
      "learning_rate": 2.7301931766992916e-05,
      "loss": 0.0121,
      "step": 2880
    },
    {
      "epoch": 2.6024313372354797,
      "grad_norm": 0.04346819221973419,
      "learning_rate": 2.6123353321936853e-05,
      "loss": 0.007,
      "step": 2890
    },
    {
      "epoch": 2.6114362899594776,
      "grad_norm": 0.010163855738937855,
      "learning_rate": 2.496937464854371e-05,
      "loss": 0.0064,
      "step": 2900
    },
    {
      "epoch": 2.620441242683476,
      "grad_norm": 0.01006612740457058,
      "learning_rate": 2.384012254705048e-05,
      "loss": 0.0059,
      "step": 2910
    },
    {
      "epoch": 2.629446195407474,
      "grad_norm": 0.008285782299935818,
      "learning_rate": 2.273572110071592e-05,
      "loss": 0.0022,
      "step": 2920
    },
    {
      "epoch": 2.6384511481314723,
      "grad_norm": 0.02291720360517502,
      "learning_rate": 2.1656291662185635e-05,
      "loss": 0.0055,
      "step": 2930
    },
    {
      "epoch": 2.6474561008554707,
      "grad_norm": 0.025554517284035683,
      "learning_rate": 2.0601952840158366e-05,
      "loss": 0.0078,
      "step": 2940
    },
    {
      "epoch": 2.6564610535794686,
      "grad_norm": 0.16730380058288574,
      "learning_rate": 1.9572820486352727e-05,
      "loss": 0.0043,
      "step": 2950
    },
    {
      "epoch": 2.665466006303467,
      "grad_norm": 0.03495648875832558,
      "learning_rate": 1.8569007682777416e-05,
      "loss": 0.0053,
      "step": 2960
    },
    {
      "epoch": 2.674470959027465,
      "grad_norm": 0.004930534865707159,
      "learning_rate": 1.7590624729305698e-05,
      "loss": 0.0008,
      "step": 2970
    },
    {
      "epoch": 2.6834759117514633,
      "grad_norm": 0.06121648848056793,
      "learning_rate": 1.6637779131555676e-05,
      "loss": 0.0017,
      "step": 2980
    },
    {
      "epoch": 2.6924808644754616,
      "grad_norm": 0.005944771692156792,
      "learning_rate": 1.571057558907707e-05,
      "loss": 0.0031,
      "step": 2990
    },
    {
      "epoch": 2.7014858171994596,
      "grad_norm": 0.08541902899742126,
      "learning_rate": 1.4809115983847265e-05,
      "loss": 0.0011,
      "step": 3000
    },
    {
      "epoch": 2.710490769923458,
      "grad_norm": 0.014513535425066948,
      "learning_rate": 1.3933499369076008e-05,
      "loss": 0.0013,
      "step": 3010
    },
    {
      "epoch": 2.719495722647456,
      "grad_norm": 0.08329233527183533,
      "learning_rate": 1.3083821958321585e-05,
      "loss": 0.003,
      "step": 3020
    },
    {
      "epoch": 2.7285006753714542,
      "grad_norm": 0.1103653833270073,
      "learning_rate": 1.226017711491867e-05,
      "loss": 0.0056,
      "step": 3030
    },
    {
      "epoch": 2.7375056280954526,
      "grad_norm": 0.002001367276534438,
      "learning_rate": 1.146265534171953e-05,
      "loss": 0.0013,
      "step": 3040
    },
    {
      "epoch": 2.7465105808194505,
      "grad_norm": 0.005002014338970184,
      "learning_rate": 1.0691344271149455e-05,
      "loss": 0.0019,
      "step": 3050
    },
    {
      "epoch": 2.755515533543449,
      "grad_norm": 0.004009018652141094,
      "learning_rate": 9.946328655577624e-06,
      "loss": 0.0011,
      "step": 3060
    },
    {
      "epoch": 2.764520486267447,
      "grad_norm": 0.971295952796936,
      "learning_rate": 9.227690358004581e-06,
      "loss": 0.0054,
      "step": 3070
    },
    {
      "epoch": 2.773525438991445,
      "grad_norm": 0.07927243411540985,
      "learning_rate": 8.53550834306674e-06,
      "loss": 0.004,
      "step": 3080
    },
    {
      "epoch": 2.7825303917154436,
      "grad_norm": 0.0031901441980153322,
      "learning_rate": 7.869858668360042e-06,
      "loss": 0.0043,
      "step": 3090
    },
    {
      "epoch": 2.791535344439442,
      "grad_norm": 0.009541003964841366,
      "learning_rate": 7.2308144760823704e-06,
      "loss": 0.0048,
      "step": 3100
    },
    {
      "epoch": 2.80054029716344,
      "grad_norm": 0.48174649477005005,
      "learning_rate": 6.618445984996863e-06,
      "loss": 0.005,
      "step": 3110
    },
    {
      "epoch": 2.8095452498874383,
      "grad_norm": 0.003029721789062023,
      "learning_rate": 6.032820482716001e-06,
      "loss": 0.0005,
      "step": 3120
    },
    {
      "epoch": 2.818550202611436,
      "grad_norm": 0.02130436897277832,
      "learning_rate": 5.474002318308108e-06,
      "loss": 0.0044,
      "step": 3130
    },
    {
      "epoch": 2.8275551553354346,
      "grad_norm": 0.033874623477458954,
      "learning_rate": 4.942052895226617e-06,
      "loss": 0.0066,
      "step": 3140
    },
    {
      "epoch": 2.836560108059433,
      "grad_norm": 0.0032497411593794823,
      "learning_rate": 4.437030664562969e-06,
      "loss": 0.0047,
      "step": 3150
    },
    {
      "epoch": 2.845565060783431,
      "grad_norm": 0.4831959009170532,
      "learning_rate": 3.958991118623967e-06,
      "loss": 0.006,
      "step": 3160
    },
    {
      "epoch": 2.8545700135074292,
      "grad_norm": 0.7759789824485779,
      "learning_rate": 3.5079867848341606e-06,
      "loss": 0.0057,
      "step": 3170
    },
    {
      "epoch": 2.863574966231427,
      "grad_norm": 0.13120326399803162,
      "learning_rate": 3.084067219964182e-06,
      "loss": 0.0015,
      "step": 3180
    },
    {
      "epoch": 2.8725799189554255,
      "grad_norm": 0.004213833715766668,
      "learning_rate": 2.6872790046854057e-06,
      "loss": 0.0021,
      "step": 3190
    },
    {
      "epoch": 2.881584871679424,
      "grad_norm": 0.04227719083428383,
      "learning_rate": 2.3176657384515464e-06,
      "loss": 0.002,
      "step": 3200
    },
    {
      "epoch": 2.890589824403422,
      "grad_norm": 0.038292016834020615,
      "learning_rate": 1.9752680347078776e-06,
      "loss": 0.0015,
      "step": 3210
    },
    {
      "epoch": 2.89959477712742,
      "grad_norm": 0.7823628187179565,
      "learning_rate": 1.660123516428802e-06,
      "loss": 0.0033,
      "step": 3220
    },
    {
      "epoch": 2.908599729851418,
      "grad_norm": 0.20850136876106262,
      "learning_rate": 1.3722668119835746e-06,
      "loss": 0.0009,
      "step": 3230
    },
    {
      "epoch": 2.9176046825754165,
      "grad_norm": 0.012090394273400307,
      "learning_rate": 1.1117295513313475e-06,
      "loss": 0.0079,
      "step": 3240
    },
    {
      "epoch": 2.926609635299415,
      "grad_norm": 0.570560872554779,
      "learning_rate": 8.785403625457822e-07,
      "loss": 0.0023,
      "step": 3250
    },
    {
      "epoch": 2.935614588023413,
      "grad_norm": 0.02576838992536068,
      "learning_rate": 6.727248686692056e-07,
      "loss": 0.001,
      "step": 3260
    },
    {
      "epoch": 2.944619540747411,
      "grad_norm": 0.03337912634015083,
      "learning_rate": 4.943056848972228e-07,
      "loss": 0.0022,
      "step": 3270
    },
    {
      "epoch": 2.953624493471409,
      "grad_norm": 0.02166174352169037,
      "learning_rate": 3.4330241609367707e-07,
      "loss": 0.0033,
      "step": 3280
    },
    {
      "epoch": 2.9626294461954075,
      "grad_norm": 0.03678569197654724,
      "learning_rate": 2.1973165463654022e-07,
      "loss": 0.0041,
      "step": 3290
    },
    {
      "epoch": 2.971634398919406,
      "grad_norm": 0.04273456707596779,
      "learning_rate": 1.2360697859462033e-07,
      "loss": 0.0033,
      "step": 3300
    },
    {
      "epoch": 2.9806393516434038,
      "grad_norm": 0.006500550080090761,
      "learning_rate": 5.493895023564455e-08,
      "loss": 0.0033,
      "step": 3310
    },
    {
      "epoch": 2.989644304367402,
      "grad_norm": 0.005440656561404467,
      "learning_rate": 1.3735114865659482e-08,
      "loss": 0.0014,
      "step": 3320
    },
    {
      "epoch": 2.9986492570914,
      "grad_norm": 0.08776138722896576,
      "learning_rate": 0.0,
      "loss": 0.0057,
      "step": 3330
    },
    {
      "epoch": 2.9986492570914,
      "eval_loss": 0.02080639824271202,
      "eval_runtime": 367.2176,
      "eval_samples_per_second": 3.399,
      "eval_steps_per_second": 0.85,
      "step": 3330
    },
    {
      "epoch": 2.9986492570914,
      "step": 3330,
      "total_flos": 1.4373461054862655e+19,
      "train_loss": 0.10910700373687186,
      "train_runtime": 87855.1134,
      "train_samples_per_second": 1.213,
      "train_steps_per_second": 0.038
    }
  ],
  "logging_steps": 10,
  "max_steps": 3330,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4373461054862655e+19,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
